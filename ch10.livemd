<!-- livebook:{"persist_outputs":true} -->

# Chapter 10. Forecast the Future

```elixir
Mix.install(
  [
    # Explorer, Elixir’s DataFrame library.
    {:explorer, "~> 0.5"},
    {:nx, "~> 0.6"},
    {:exla, "~> 0.6"},
    {:axon, "~> 0.5"},
    {:vega_lite, "~> 0.1.6"},
    {:kino, "~> 0.8.0"},
    {:kino_vega_lite, "~> 0.1.7"}
  ],
  config: [nx: [default_backend: {EXLA.Backend, client: :cuda}]]
)

# export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
# export XLA_TARGET=cuda120 
# Nx.global_default_backend(EXLA.Backend)
Nx.Defn.global_default_options(compiler: EXLA)
```

## Introduction

Anything with temporal nature presents
challenges for traditional feed-forward networks and is a well-suited challenge
for recurrent neural networks.

```elixir
alias VegaLite, as: Vl
```

<!-- livebook:{"output":true} -->

```
VegaLite
```

## Predicting Stock Prices

```elixir
notebook_path = "/home/alde/Documents/MyDevelopment/Machine_Learning_in_Elixir"
csv_file = "all_stocks_2006-01-01_to_2018-01-01.csv"

csv_path =
  notebook_path 
  |> Path.join("training_data") 
  |> Path.join(csv_file)

df = Explorer.DataFrame.from_csv!(csv_path, parse_dates: true)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[93612 x 7]
  Date date [2006-01-03, 2006-01-04, 2006-01-05, 2006-01-06, 2006-01-09, ...]
  Open f64 [77.76, 79.49, 78.41, 78.64, 78.5, ...]
  High f64 [79.35, 79.49, 78.65, 78.9, 79.83, ...]
  Low f64 [77.24, 78.25, 77.56, 77.64, 78.46, ...]
  Close f64 [79.11, 78.71, 77.99, 78.63, 79.02, ...]
  Volume s64 [3117200, 2558000, 2529500, 2479500, 1845600, ...]
  Name string ["MMM", "MMM", "MMM", "MMM", "MMM", ...]
>
```

Autoregression means you’re going to predict future values from existing
values.

```elixir
df = Explorer.DataFrame.select(df, ["Date", "Close", "Name"])
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[93612 x 3]
  Date date [2006-01-03, 2006-01-04, 2006-01-05, 2006-01-06, 2006-01-09, ...]
  Close f64 [79.11, 78.71, 77.99, 78.63, 79.02, ...]
  Name string ["MMM", "MMM", "MMM", "MMM", "MMM", ...]
>
```

```elixir
Vl.new(title: "DJIA Stock Prices", width: 640, height: 480)
|> Vl.data_from_values(Explorer.DataFrame.to_columns(df))
|> Vl.mark(:line)
|> Vl.encode_field(:x, "Date", type: :temporal)
|> Vl.encode_field(:y, "Close", type: :quantitative)
|> Vl.encode_field(:color, "Name", type: :nominal)
|> Kino.VegaLite.new()
```

```elixir
aapl_df =
  Explorer.DataFrame.filter_with(df, fn df ->
    Explorer.Series.equal(df["Name"], "AAPL")
  end)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[3019 x 3]
  Date date [2006-01-03, 2006-01-04, 2006-01-05, 2006-01-06, 2006-01-09, ...]
  Close f64 [10.68, 10.71, 10.63, 10.9, 10.86, ...]
  Name string ["AAPL", "AAPL", "AAPL", "AAPL", "AAPL", ...]
>
```

```elixir
Vl.new(title: "AAPL Stock Price", width: 640, height: 480)
|> Vl.data_from_values(Explorer.DataFrame.to_columns(aapl_df))
|> Vl.mark(:line)
|> Vl.encode_field(:x, "Date", type: :temporal)
|> Vl.encode_field(:y, "Close", type: :quantitative)
|> Kino.VegaLite.new()
```

```elixir
normalized_aapl_df =
  Explorer.DataFrame.mutate_with(aapl_df, fn df ->
    var = Explorer.Series.variance(df["Close"])
    mean = Explorer.Series.mean(df["Close"])
    centered = Explorer.Series.subtract(df["Close"], mean)
    norm = Explorer.Series.divide(centered, var)
    [Close: norm]
  end)

    
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[3019 x 3]
  Date date [2006-01-03, 2006-01-04, 2006-01-05, 2006-01-06, 2006-01-09, ...]
  Close f64 [-0.02721604325477659, -0.02720091843889246, -0.027241251281250132, -0.027105127938293,
   -0.027125294359471832, ...]
  Name string ["AAPL", "AAPL", "AAPL", "AAPL", "AAPL", ...]
>
```

```elixir
defmodule Data do
  def window(inputs, window_size, target_window_size) do
    inputs
    |> Stream.chunk_every(window_size + target_window_size, 1, :discard)
    |> Stream.map(fn window ->
      features =
        window
        |> Enum.take(window_size)
        |> Nx.tensor()
        |> Nx.new_axis(1)

      targets =
        window
        |> Enum.drop(window_size)
        |> Nx.tensor()
        |> Nx.new_axis(1)

      {features, targets}
    end)
  end

  def batch(inputs, batch_size) do
    inputs
    |> Stream.chunk_every(batch_size, batch_size, :discard)
    |> Stream.map(fn windows ->
      {features, targets} = Enum.unzip(windows)
      {Nx.stack(features), Nx.stack(targets)}
    end)
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Data, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:batch, 2}}
```

With a timeseries dataset, you have a bunch of potentially overlapping dependencies, which means you can leak information about your test set into the training process. Leakage isn’t good, and it can result in overconfidence and deployment of bad models.

A challenge with time-series analysis is that there are often confounding
variables at play, and without access to that information, it can be difficult
to get an accurate model.

One consideration when doing time-series analysis is that
you will constantly need to re-train and update trends.

```elixir
train_df =
  Explorer.DataFrame.filter_with(normalized_aapl_df, fn df ->
    Explorer.Series.less(df["Date"], Date.new!(2016, 1, 1))
  end)

test_df =
  Explorer.DataFrame.filter_with(normalized_aapl_df, fn df ->
    Explorer.Series.greater_equal(df["Date"], Date.new!(2016, 1, 1))
  end)
```

<!-- livebook:{"output":true} -->

```
#Explorer.DataFrame<
  Polars[503 x 3]
  Date date [2016-01-04, 2016-01-05, 2016-01-06, 2016-01-07, 2016-01-08, ...]
  Close f64 [0.02051283407023046, 0.01918185027242737, 0.01816848760819093, 0.01602580535793974,
   0.016282927227969878, ...]
  Name string ["AAPL", "AAPL", "AAPL", "AAPL", "AAPL", ...]
>
```

```elixir
window_size = 5
batch_size = 32
train_prices = Explorer.Series.to_list(train_df["Close"])
test_prices = Explorer.Series.to_list(test_df["Close"])

single_step_train_data =
  train_prices
  |> Data.window(window_size, 1)
  |> Data.batch(batch_size)

single_step_test_data =
  test_prices
  |> Data.window(window_size, 1)
  |> Data.batch(batch_size)
```

<!-- livebook:{"output":true} -->

```
#Stream<[
  enum: #Stream<[
    enum: #Stream<[
      enum: [0.02051283407023046, 0.01918185027242737, 0.01816848760819093, 0.01602580535793974,
       0.016282927227969878, 0.017074459259239144, 0.017795408816382485, 0.016499716255642356,
       0.017573578183415303, 0.01636863451797993, 0.016131679069128622, 0.016197219937959837,
       0.015950181278519108, 0.018531483189409954, 0.017533245341057633, 0.01781053363226661,
       0.014498198953643012, 0.014835986508388493, 0.016474508229168815, 0.016015722147350326,
       0.015032609114882132, 0.015975389304992652, 0.016101429437360367, 0.014800695271325529,
       0.015299814195501693, 0.015289730984912271, 0.014926735403693247, 0.014639363901894855,
       0.014785570455441403, 0.016121595858539204, 0.016867753442156092, 0.015930014857340278,
       0.01581909954085669, 0.01624259438561221, 0.015138482826071012, 0.015849349172624934,
       0.01618209512207571, 0.016257719201496337, 0.01614680388501275, 0.01808278031818088,
       0.018193695634664473, 0.01857181603176762, 0.019333098431268635, 0.018758355427671847,
       0.018334860582916314, 0.018380235030568695, 0.018405443057042236, ...],
      funs: [#Function<3.53678557/1 in Stream.chunk_while/4>]
    ]>,
    funs: [#Function<48.53678557/1 in Stream.map/2>,
     #Function<3.53678557/1 in Stream.chunk_while/4>]
  ]>,
  funs: [#Function<48.53678557/1 in Stream.map/2>]
]>
```

```elixir
Enum.take(single_step_train_data, 1)
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Tensor<
     f32[32][5][1]
     EXLA.Backend<cuda:0, 0.1404480797.1337589794.260889>
     [
       [
         [-0.027216043323278427],
         [-0.027200918644666672],
         [-0.02724125050008297],
         [-0.02710512839257717],
         [-0.027125295251607895]
       ],
       [
         [-0.027200918644666672],
         [-0.02724125050008297],
         [-0.02710512839257717],
         [-0.027125295251607895],
         [-0.026777423918247223]
       ],
       [
         [-0.02724125050008297],
         [-0.02710512839257717],
         [-0.027125295251607895],
         [-0.026777423918247223],
         [-0.026555592194199562]
       ],
       [
         [-0.02710512839257717],
         [-0.027125295251607895],
         [-0.026777423918247223],
         [-0.026555592194199562],
         [-0.02653038501739502]
       ],
       [
         [-0.027125295251607895],
         [-0.026777423918247223],
         [-0.026555592194199562],
         [-0.02653038501739502],
         [-0.02643459476530552]
       ],
       [
         [-0.026777423918247223],
         [-0.026555592194199562],
         [-0.02653038501739502],
         [-0.02643459476530552],
         [-0.02650013566017151]
       ],
       [
         [-0.026555592194199562],
         [-0.02653038501739502],
         [-0.02643459476530552],
         [-0.02650013566017151],
         [-0.026661466807127]
       ],
       [
         [-0.02653038501739502],
         [-0.02643459476530552],
         [-0.02650013566017151],
         [-0.026661466807127],
         [-0.026908505707979202]
       ],
       [
         [-0.02643459476530552],
         [-0.02650013566017151],
         [-0.026661466807127],
         [-0.026908505707979202],
         [-0.027120253071188927]
       ],
       [
         [-0.02650013566017151],
         [-0.026661466807127],
         [-0.026908505707979202],
         ...
       ],
       ...
     ]
   >,
   #Nx.Tensor<
     f32[32][1][1]
     EXLA.Backend<cuda:0, 0.1404480797.1337589794.260922>
     [
       [
         [-0.026777423918247223]
       ],
       [
         [-0.026555592194199562]
       ],
       [
         [-0.02653038501739502]
       ],
       [
         [-0.02643459476530552]
       ],
       [
         [-0.02650013566017151]
       ],
       [
         [-0.026661466807127]
       ],
       [
         [-0.026908505707979202]
       ],
       [
         [-0.027120253071188927]
       ],
       [
         [-0.027004295960068703]
       ],
       [
         [-0.027125295251607895]
       ],
       [
         [-0.027256375178694725]
       ],
       [
         [-0.027392499148845673]
       ],
       [
         [-0.027412666007876396]
       ],
       [
         [-0.027200918644666672]
       ],
       [
         [-0.027160584926605225]
       ],
       [
         [-0.02717066928744316]
       ],
       [
         [-0.027407623827457428]
       ],
       [
         [-0.02742779068648815]
       ],
       [
         [-0.0277554951608181]
       ],
       [
         [-0.027730286121368408]
       ],
       [
         [-0.027644580230116844]
       ],
       [
         [-0.02792186848819256]
       ],
       [
         [-0.027750452980399132]
       ],
       [
         [-0.027942035347223282]
       ],
       [
         [-0.027730286121368408]
       ],
       [
         [-0.027614330872893333]
       ],
       [
         [-0.027518538758158684]
       ],
       [
         [-0.027538705617189407]
       ],
       [
         [-0.02762441337108612]
       ],
       [
         [-0.02746308222413063]
       ],
       [
         [-0.02743283286690712]
       ],
       [
         [-0.027452997863292694]
       ]
     ]
   >}
]
```

## Using CNNs for Single Step Prediction

The properties that make convolutional neural networks useful for image
data also make them useful for time-series data. With time-series data, local
relationships matter because adjacent timesteps are likely related.

In situations where you have multiple features per time-window, convolutions are capable of extracting information both across your feature dimension and temporal dimension.

Feed-forward networks fall short here because they don’t have the same built-in assumptions about input data as convolutions.

Most commonly, time-series models use mean squared error or root-mean
squared error as the loss function.

Both of these loss functions penalize large errors, which is beneficial when you’re trying to closely predict the correct next values in a sequence.

It’s common to use mean-absolute error because it’s more readily interpretable than MSE.

Single-step predictions are generally easier than multi-step predictions because
you don’t drift too far from the initial conditions you’re modeling against.

```elixir
cnn_model =
  Axon.input("stock_price")
  |> Axon.nx(&Nx.new_axis(&1, -1))
  |> Axon.conv(32, kernel_size: {window_size, 1}, activation: :relu)
  |> Axon.dense(32, activation: :relu)
  |> Axon.dense(1)

template = Nx.template({32, 10, 1}, :f32)
Axon.Display.as_graph(cnn_model, template)
```

```elixir
cnn_trained_model_state =
cnn_model
|> Axon.Loop.trainer(:mean_squared_error, :adam)
|> Axon.Loop.metric(:mean_absolute_error)
|> Axon.Loop.run(single_step_train_data, %{}, epochs: 10, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

22:49:05.705 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 50, loss: 0.0000725 mean_absolute_error: 0.0060588
Epoch: 1, Batch: 72, loss: 0.0001352 mean_absolute_error: 0.0094406
Epoch: 2, Batch: 44, loss: 0.0001542 mean_absolute_error: 0.0113930
Epoch: 3, Batch: 66, loss: 0.0001172 mean_absolute_error: 0.0051769
Epoch: 4, Batch: 38, loss: 0.0001096 mean_absolute_error: 0.0065860
Epoch: 5, Batch: 60, loss: 0.0000925 mean_absolute_error: 0.0047765
Epoch: 6, Batch: 32, loss: 0.0000868 mean_absolute_error: 0.0055139
Epoch: 7, Batch: 54, loss: 0.0000764 mean_absolute_error: 0.0041475
Epoch: 8, Batch: 76, loss: 0.0000675 mean_absolute_error: 0.0028349
Epoch: 9, Batch: 48, loss: 0.0000640 mean_absolute_error: 0.0028764
```

<!-- livebook:{"output":true} -->

```
%{
  "conv_0" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.130131>
      [-0.004409558139741421, -0.009908050298690796, -0.006246498785912991, -0.011087640188634396, -0.0027498353738337755, -0.0036197297740727663, -0.010914104990661144, -0.009279098361730576, 0.021876033395528793, 0.020785734057426453, -0.0053697084076702595, 0.030585618689656258, -0.009016593918204308, -0.008728004060685635, -0.006005314644426107, -0.009073060937225819, 0.0010286589385941625, -0.009644147008657455, 0.02579512819647789, -0.00796926487237215, 0.008263945579528809, -0.011179899796843529, -0.011643302626907825, -0.00773121090605855, -0.01012479979544878, -0.006005449220538139, 0.02084381692111492, -0.011847426183521748, -8.958638063631952e-4, 0.020936960354447365, -0.007289621513336897, 0.008664986118674278]
    >,
    "kernel" => #Nx.Tensor<
      f32[5][1][1][32]
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.130132>
      [
        [
          [
            [-0.008729991503059864, -0.07854350656270981, 0.013638263568282127, -0.042738914489746094, -0.02922920323908329, -0.011928721331059933, -0.07803609222173691, -0.10646765679121017, 0.17655305564403534, -0.2316162884235382, 0.07568002492189407, 0.26880520582199097, -0.15342862904071808, 0.17267625033855438, 0.07741320878267288, 0.018003150820732117, -0.18916653096675873, 0.12377267330884933, 0.20146141946315765, -0.1506013125181198, 0.1323123276233673, 0.13675378262996674, 0.1567612588405609, 0.12728381156921387, -0.048179060220718384, -0.011706264689564705, 0.07151247560977936, 0.085239939391613, 0.0923287495970726, 0.11207658797502518, -0.16649624705314636, -0.15866641700267792]
          ]
        ],
        [
          [
            [-0.09380193799734116, 0.1010112464427948, -0.16458237171173096, -0.11029887944459915, -0.16736480593681335, -0.07922513037919998, -0.14986373484134674, -0.005718000698834658, -0.03998785465955734, -0.2331957370042801, -0.1094425767660141, 0.2297850400209427, 0.03982699662446976, -0.1693699061870575, 0.19193610548973083, ...]
          ]
        ],
        ...
      ]
    >
  },
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[32]
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.130133>
      [-0.012502109631896019, -0.00381491775624454, -0.007259319070726633, 0.004436586983501911, 0.0, -0.009064540266990662, 0.02016640640795231, 0.011254082433879375, 0.0020715794526040554, -0.008384195156395435, -0.01386192999780178, -0.023742493242025375, -0.010465784929692745, -0.020518023520708084, -0.007656830362975597, 0.005513712298125029, -0.015097049064934254, -0.0047234357334673405, 0.0, -0.009053675457835197, -0.011857794597744942, -0.00884935725480318, -0.00838493648916483, -0.018796322867274284, 0.01086964551359415, 0.003918954636901617, -0.009267418645322323, -0.005611460190266371, -0.012284099124372005, 0.01338932290673256, -0.012926056981086731, -0.010014534927904606]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][32]
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.130134>
      [
        [-0.1648743897676468, 0.2733580768108368, 0.2345501184463501, -0.20536008477210999, 0.1257382333278656, -0.10560725629329681, 0.11826342344284058, 0.2874757647514343, 0.22284787893295288, 0.052617598325014114, -0.2325584590435028, -0.2494828701019287, 0.08524975925683975, -0.2660391926765442, 0.22269508242607117, 0.006195416674017906, -0.15612120926380157, -0.03251659497618675, 0.02446603775024414, 0.17438296973705292, 0.1517067700624466, -0.2571975886821747, -0.275764137506485, 0.13625846803188324, -0.06666257977485657, -0.1957380622625351, 0.25486284494400024, -0.01969638094305992, -0.26253822445869446, 0.2885768711566925, 0.20000897347927094, 0.1859680563211441],
        [0.10934816300868988, 0.2887261211872101, 0.0028577770572155714, -0.2202799916267395, -0.024620801210403442, 0.28153252601623535, -0.28519997000694275, -0.19523563981056213, 0.024510657414793968, 0.2522112727165222, 0.24182744324207306, -0.052764344960451126, 0.22247253358364105, -0.27068206667900085, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[1]
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.130135>
      [-0.009006922133266926]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][1]
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.130136>
      [
        [0.1316811740398407],
        [-0.21309468150138855],
        [0.05098110809922218],
        [0.11578480154275894],
        [-0.3802316188812256],
        [-0.15999361872673035],
        [-0.37596604228019714],
        [0.2605763077735901],
        [0.12458443641662598],
        [0.2641994059085846],
        [0.3316980302333832],
        [0.13438215851783752],
        [-0.018620263785123825],
        [0.12296081334352493],
        [-0.3933921754360199],
        [0.2940464913845062],
        [-0.3294847309589386],
        [-0.0013171321479603648],
        [-0.2403365522623062],
        [0.12201020121574402],
        [0.22011283040046692],
        [-0.4000893831253052],
        [0.40556782484054565],
        [0.34153273701667786],
        [0.2886366546154022],
        [-0.014985587447881699],
        [0.00817825086414814],
        [-0.07454731315374374],
        [-0.05016913264989853],
        [-0.11424396932125092],
        [0.0021714726462960243],
        [-3.342996205901727e-5]
      ]
    >
  }
}
```

```elixir
cnn_model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(:mean_absolute_error)
|> Axon.Loop.run(single_step_test_data, cnn_trained_model_state, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

22:49:17.644 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Batch: 14, mean_absolute_error: 0.0048055
```

<!-- livebook:{"output":true} -->

```
%{
  0 => %{
    "mean_absolute_error" => #Nx.Tensor<
      f32
      EXLA.Backend<cuda:0, 0.1404480797.1337851938.133478>
      0.004805455915629864
    >
  }
}
```

```elixir
0.006610621698200703
|> Kernel.*(:math.sqrt(Explorer.Series.variance(aapl_df["Close"])))
|> Kernel.+(Explorer.Series.mean(aapl_df["Close"]))

# your model had an absolute error of $64 / $ 65
# off the next day’s closing stock price
```

<!-- livebook:{"output":true} -->

```
64.95730529690192
```

```elixir
defmodule Analysis do
  def visualize_predictions(
        model,
        model_state,
        prices,
        window_size,
        target_window_size,
        batch_size
        #variance,
        #mean
      ) do
    {_, predict_fn} = Axon.build(model, compiler: EXLA)

    windows =
      prices
      |> Data.window(window_size, target_window_size)
      |> Data.batch(batch_size)
      |> Stream.map(&elem(&1, 0))

    # TODO: Add desnormalization.
    predicted =
      Enum.flat_map(windows, fn window ->
        predict_fn.(model_state, window) 
        #|> Nx.multiply(variance)
        #|> Nx.add(mean)
        |> Nx.to_flat_list()
      end)

    predicted = List.duplicate(nil, 10) ++ predicted

    types =
      List.duplicate("AAPL", length(prices)) ++
        List.duplicate("Predicted", length(prices))

    days =
      Enum.to_list(0..(length(prices) - 1)) ++
        Enum.to_list(0..(length(prices) - 1))

    prices = prices ++ predicted

    plot(
      %{
        "day" => days,
        "prices" => prices,
        "types" => types
      },
      "AAPL Stock Price vs. Predicted, CNN Single-Shot"
    )
  end

  defp plot(values, title) do
    Vl.new(title: title, width: 640, height: 480)
    |> Vl.data_from_values(values)
    |> Vl.mark(:line)
    |> Vl.encode_field(:x, "day", type: :temporal)
    |> Vl.encode_field(:y, "prices", type: :quantitative)
    |> Vl.encode_field(:color, "types", type: :nominal)
    |> Kino.VegaLite.new()
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Analysis, <<70, 79, 82, 49, 0, 0, 15, ...>>, {:plot, 2}}
```

```elixir
Analysis.visualize_predictions(
  cnn_model,
  cnn_trained_model_state,
  Explorer.Series.to_list(normalized_aapl_df["Close"]),
  window_size,
  1,
  batch_size
)
```

## Using RNNs for Time Series Prediction

The reason recurrent neural networks are so powerful with text and timeseries is because they model temporal dependencies.

```elixir
rnn_model =
  Axon.input("stock_prices")
  |> Axon.lstm(32) # {secuence, state}
  |> elem(0)
  |> Axon.nx(& &1[[0..-1//1, -1, 0..-1//1]])
  |> Axon.dense(1)

template = Nx.template({32, 10, 1}, :f32)
Axon.Display.as_graph(rnn_model, template)
```

```elixir
rnn_trained_model_state =
  rnn_model
  |> Axon.Loop.trainer(:mean_squared_error, :adam)
  |> Axon.Loop.metric(:mean_absolute_error)
  |> Axon.Loop.run(single_step_train_data, %{}, epochs: 50, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

23:06:20.623 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 50, loss: 0.0002247 mean_absolute_error: 0.0113152
Epoch: 1, Batch: 72, loss: 0.0003021 mean_absolute_error: 0.0135855
Epoch: 2, Batch: 44, loss: 0.0003154 mean_absolute_error: 0.0144157
Epoch: 3, Batch: 66, loss: 0.0002660 mean_absolute_error: 0.0104020
Epoch: 4, Batch: 38, loss: 0.0002614 mean_absolute_error: 0.0127070
Epoch: 5, Batch: 60, loss: 0.0002275 mean_absolute_error: 0.0088169
Epoch: 6, Batch: 32, loss: 0.0002195 mean_absolute_error: 0.0111166
Epoch: 7, Batch: 54, loss: 0.0001940 mean_absolute_error: 0.0070202
Epoch: 8, Batch: 76, loss: 0.0001736 mean_absolute_error: 0.0053724
Epoch: 9, Batch: 48, loss: 0.0001656 mean_absolute_error: 0.0053155
Epoch: 10, Batch: 70, loss: 0.0001492 mean_absolute_error: 0.0039588
Epoch: 11, Batch: 42, loss: 0.0001424 mean_absolute_error: 0.0038762
Epoch: 12, Batch: 64, loss: 0.0001295 mean_absolute_error: 0.0029091
Epoch: 13, Batch: 36, loss: 0.0001239 mean_absolute_error: 0.0027950
Epoch: 14, Batch: 58, loss: 0.0001138 mean_absolute_error: 0.0022280
Epoch: 15, Batch: 30, loss: 0.0001094 mean_absolute_error: 0.0021595
Epoch: 16, Batch: 52, loss: 0.0001015 mean_absolute_error: 0.0018104
Epoch: 17, Batch: 74, loss: 0.0000947 mean_absolute_error: 0.0017879
Epoch: 18, Batch: 46, loss: 0.0000915 mean_absolute_error: 0.0016134
Epoch: 19, Batch: 68, loss: 0.0000860 mean_absolute_error: 0.0016687
Epoch: 20, Batch: 40, loss: 0.0000835 mean_absolute_error: 0.0016318
Epoch: 21, Batch: 62, loss: 0.0000789 mean_absolute_error: 0.0016639
Epoch: 22, Batch: 34, loss: 0.0000768 mean_absolute_error: 0.0017632
Epoch: 23, Batch: 56, loss: 0.0000729 mean_absolute_error: 0.0017149
Epoch: 24, Batch: 28, loss: 0.0000711 mean_absolute_error: 0.0019385
Epoch: 25, Batch: 50, loss: 0.0000678 mean_absolute_error: 0.0017519
Epoch: 26, Batch: 72, loss: 0.0000648 mean_absolute_error: 0.0018083
Epoch: 27, Batch: 44, loss: 0.0000635 mean_absolute_error: 0.0018411
Epoch: 28, Batch: 66, loss: 0.0000609 mean_absolute_error: 0.0018909
Epoch: 29, Batch: 38, loss: 0.0000597 mean_absolute_error: 0.0021340
Epoch: 30, Batch: 60, loss: 0.0000575 mean_absolute_error: 0.0021332
Epoch: 31, Batch: 32, loss: 0.0000565 mean_absolute_error: 0.0026950
Epoch: 32, Batch: 54, loss: 0.0000546 mean_absolute_error: 0.0024544
Epoch: 33, Batch: 76, loss: 0.0000529 mean_absolute_error: 0.0025513
Epoch: 34, Batch: 48, loss: 0.0000522 mean_absolute_error: 0.0028949
Epoch: 35, Batch: 70, loss: 0.0000508 mean_absolute_error: 0.0028809
Epoch: 36, Batch: 42, loss: 0.0000502 mean_absolute_error: 0.0034348
Epoch: 37, Batch: 64, loss: 0.0000489 mean_absolute_error: 0.0030559
Epoch: 38, Batch: 36, loss: 0.0000485 mean_absolute_error: 0.0039469
Epoch: 39, Batch: 58, loss: 0.0000473 mean_absolute_error: 0.0031853
Epoch: 40, Batch: 30, loss: 0.0000469 mean_absolute_error: 0.0040290
Epoch: 41, Batch: 52, loss: 0.0000458 mean_absolute_error: 0.0030629
Epoch: 42, Batch: 74, loss: 0.0000447 mean_absolute_error: 0.0026958
Epoch: 43, Batch: 46, loss: 0.0000443 mean_absolute_error: 0.0029037
Epoch: 44, Batch: 68, loss: 0.0000433 mean_absolute_error: 0.0025439
Epoch: 45, Batch: 40, loss: 0.0000429 mean_absolute_error: 0.0028590
Epoch: 46, Batch: 62, loss: 0.0000419 mean_absolute_error: 0.0024570
Epoch: 47, Batch: 34, loss: 0.0000415 mean_absolute_error: 0.0029885
Epoch: 48, Batch: 56, loss: 0.0000406 mean_absolute_error: 0.0024672
Epoch: 49, Batch: 28, loss: 0.0000402 mean_absolute_error: 0.0030644
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[1]
      EXLA.Backend<cuda:0, 0.1404480797.1338638370.72075>
      [-0.0010127904824912548]
    >,
    "kernel" => #Nx.Tensor<
      f32[32][1]
      EXLA.Backend<cuda:0, 0.1404480797.1338638370.72076>
      [
        [0.07001381367444992],
        [-0.008358930237591267],
        [-0.06035745516419411],
        [-0.07316654920578003],
        [0.03490469977259636],
        [-0.09321817010641098],
        [-0.3125406801700592],
        [0.092958003282547],
        [-0.2197282314300537],
        [-0.16146546602249146],
        [0.3383268713951111],
        [0.19076645374298096],
        [0.31158503890037537],
        [0.04780799150466919],
        [0.28973186016082764],
        [0.2079845368862152],
        [0.11447557061910629],
        [0.10927660763263702],
        [-0.22267192602157593],
        [0.09785622358322144],
        [-0.20914426445960999],
        [0.3063015043735504],
        [0.30791348218917847],
        [0.3919993042945862],
        [0.02184230647981167],
        [-0.09370511770248413],
        [-0.1613159477710724],
        [0.134682759642601],
        [-0.12649095058441162],
        [0.2899077832698822],
        [-3.8812871207483113e-4],
        [-0.40282660722732544]
      ]
    >
  },
  "lstm_0" => %{
    "bias" => %{
      "bf" => #Nx.Tensor<
        f32[32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72077>
        [-0.04076290503144264, -0.08022449910640717, -0.10472065210342407, -0.025311311706900597, -0.11417188495397568, -0.14946183562278748, 0.02320939116179943, -0.18998761475086212, 0.005617815535515547, -0.13161030411720276, -0.018590111285448074, -0.13013215363025665, -0.12837935984134674, -0.12663814425468445, -0.060836099088191986, -0.0522872619330883, -0.14537663757801056, 0.005445694550871849, -0.09355846792459488, -0.008601084351539612, -0.020411048084497452, 0.06174993887543678, -0.04191968962550163, -0.10541891306638718, 0.019523167982697487, -0.0958600714802742, -0.19175828993320465, 0.03668111190199852, -0.013342457823455334, 0.015728114172816277, -0.11268728226423264, -0.0645422488451004]
      >,
      "bg" => #Nx.Tensor<
        f32[32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72078>
        [-0.002043795073404908, -0.0013332272646948695, -0.0033800352830439806, 0.001183368731290102, -0.004303060006350279, 0.005134501960128546, 8.92254407517612e-4, -0.0025248790625482798, -0.011489548720419407, 7.247250177897513e-4, 0.0013090502470731735, 0.004187020938843489, 0.004790030419826508, -0.003567802021279931, -0.0017077812226489186, 0.004050428047776222, -0.002795299980789423, 0.0026779433246701956, 0.006540508475154638, 0.012017721310257912, -0.0025965445674955845, 0.0028627952560782433, 0.004698561504483223, 0.0014264491619542241, -0.0020233935210853815, 0.002807873534038663, 0.0020714623387902975, 0.004522409290075302, -0.006465999409556389, 0.008793925866484642, -0.004025487694889307, -1.5618480392731726e-4]
      >,
      "bi" => #Nx.Tensor<
        f32[32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72079>
        [-0.1783849596977234, -0.10420618951320648, -0.17980051040649414, -0.16128064692020416, -0.24011588096618652, -0.18213249742984772, 0.15733255445957184, -0.1567855179309845, 0.1888941377401352, -0.1793326437473297, -0.14896541833877563, -0.13753630220890045, -0.009078497998416424, -0.15992245078086853, -0.03202090412378311, 0.12958525121212006, -0.26317447423934937, -0.17367751896381378, -0.04927387833595276, 0.21669943630695343, 0.01891002058982849, 0.12406753748655319, 0.20006588101387024, 0.12096027284860611, -0.03207617625594139, -0.22401952743530273, -0.23037627339363098, -0.012428026646375656, -0.07743746787309647, 0.19851039350032806, -0.16113631427288055, 0.179002046585083]
      >,
      "bo" => #Nx.Tensor<
        f32[32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72080>
        [-0.1010361835360527, -0.09638053923845291, -0.13264060020446777, -0.09657906740903854, -0.1730039119720459, -0.16203239560127258, 0.09992538392543793, -0.17630153894424438, 0.062001898884773254, -0.16985160112380981, -0.0722440704703331, -0.13939347863197327, -0.09464776515960693, -0.14830751717090607, -0.07067973166704178, 0.030922429636120796, -0.19995266199111938, -0.13053826987743378, -0.08825824409723282, 0.07579011470079422, 0.003296941053122282, 0.09870804101228714, 0.07820585370063782, 0.03073633834719658, -0.008236070163547993, -0.17592574656009674, -0.21655942499637604, 0.012580562382936478, -0.04241582006216049, 0.08806521445512772, -0.15806688368320465, 0.0354033000767231]
      >
    },
    "hidden_kernel" => %{
      "whf" => #Nx.Tensor<
        f32[32][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72081>
        [
          [0.08720177412033081, -0.27127909660339355, -0.19456297159194946, 0.22483929991722107, 0.07359404861927032, -0.3088170886039734, -0.2421637773513794, -0.05492724850773811, 0.2002425640821457, -0.4419163167476654, -0.01541057787835598, 0.20226339995861053, 0.2856515049934387, -0.13360781967639923, 0.27742284536361694, 0.32843416929244995, -0.13777142763137817, -0.18856529891490936, -0.10755855590105057, -0.003742877161130309, -0.06969483196735382, -0.0749393031001091, -0.10550929605960846, 0.1246529072523117, -0.16030392050743103, 0.025763189420104027, -0.17679966986179352, -0.13342590630054474, -0.041784293949604034, 0.018137069419026375, 0.17585645616054535, 0.08295240253210068],
          [0.14487677812576294, -0.10459219664335251, -0.2961927652359009, -0.09502220898866653, -0.23799249529838562, 0.12538038194179535, 0.28469353914260864, -0.17295876145362854, 0.03754190355539322, 0.4274238049983978, -0.18121854960918427, 0.01844334788620472, -0.001298548304475844, ...],
          ...
        ]
      >,
      "whg" => #Nx.Tensor<
        f32[32][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72082>
        [
          [0.19216908514499664, 0.35895198583602905, 0.2150917649269104, -0.21044932305812836, 0.06398580968379974, 0.010424535721540451, 0.17281043529510498, -0.1269252747297287, 1.885659439722076e-4, -0.07926509529352188, -0.022779587656259537, 0.23856663703918457, 0.2416388839483261, 0.23021815717220306, -0.09544406086206436, 0.2218848019838333, -0.05798526108264923, 0.2506089210510254, 0.2970966100692749, 0.3556479811668396, -0.10485444962978363, -0.01983972080051899, -0.18877924978733063, -0.1936374008655548, 0.20034682750701904, 0.024228885769844055, 0.10774172842502594, 0.23995040357112885, -0.06798378378152847, -0.09802737832069397, 0.12209516763687134, 0.32216915488243103],
          [-0.0714275985956192, 0.16348527371883392, 0.24978382885456085, -0.009848622605204582, 0.33550721406936646, -0.299368292093277, 0.0205812007188797, -0.1888287216424942, 0.08307559043169022, -0.16719046235084534, 0.3151003122329712, 0.05657582730054855, ...],
          ...
        ]
      >,
      "whi" => #Nx.Tensor<
        f32[32][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72083>
        [
          [-0.009915553964674473, -0.1321389377117157, -0.09372790157794952, 0.236225888133049, 0.07306894659996033, -0.21559610962867737, 0.1807275414466858, 0.04840035364031792, 0.04732966795563698, 0.07641132175922394, 0.0637105256319046, 0.1795208901166916, 0.35832875967025757, 0.02917204611003399, 0.33804163336753845, -0.11659953743219376, -0.14205846190452576, 0.06290154159069061, 0.11392094939947128, -0.04570188745856285, -0.22647902369499207, 0.0938812792301178, 0.1456165909767151, 0.25612515211105347, -0.06899154186248779, 0.22563393414020538, -0.28511083126068115, 0.10106943547725677, 0.12701156735420227, -0.12034314125776291, 0.04821410030126572, -0.08863360434770584],
          [0.09423529356718063, 0.20896998047828674, -0.22999423742294312, 0.14671333134174347, -0.04768770560622215, 0.046983279287815094, -0.2765938639640808, -0.07623710483312607, 0.3060544729232788, -0.11861804127693176, -0.2642083764076233, ...],
          ...
        ]
      >,
      "who" => #Nx.Tensor<
        f32[32][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72084>
        [
          [0.13957266509532928, 0.29860562086105347, 0.2706679701805115, -0.006902540102601051, 0.23815275728702545, -0.27102285623550415, -0.22179310023784637, 0.04842523857951164, 0.2083875983953476, -0.07828088849782944, 0.10096211731433868, -0.0078028906136751175, 0.24280479550361633, -0.03911912813782692, 0.008670511655509472, 0.2908686399459839, -0.11755354702472687, -0.1630752682685852, 0.1753774881362915, 0.13908705115318298, -0.06998778134584427, 0.3264167904853821, 0.13296376168727875, -0.10349583625793457, 0.19308751821517944, -0.10378790646791458, 0.27578994631767273, -0.03850429505109787, -0.2696598470211029, -0.14872708916664124, -0.1221599131822586, -0.07177475094795227],
          [0.1502636820077896, 0.18339820206165314, -0.17014047503471375, 0.2023608386516571, 0.14804935455322266, -0.3267865478992462, 0.07331656664609909, 0.011749425902962685, 0.16872960329055786, 0.3418285846710205, ...],
          ...
        ]
      >
    },
    "input_kernel" => %{
      "wif" => #Nx.Tensor<
        f32[1][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72085>
        [
          [0.5644716620445251, -0.349528044462204, -0.12246321886777878, -0.15325085818767548, -0.1286952793598175, -0.10733769834041595, 0.1739734560251236, 0.2867363691329956, -0.4438565969467163, 0.3749943673610687, 0.24105443060398102, 0.0580257922410965, -0.42894163727760315, -0.002059345133602619, 0.28306517004966736, 0.4481925070285797, -0.2829347848892212, -0.15444529056549072, 0.09813537448644638, 0.02425699308514595, 0.42021891474723816, 0.5512771606445312, 0.31886011362075806, -0.21401913464069366, -0.15500740706920624, 0.3703179955482483, 0.0936170145869255, -0.34323039650917053, 0.021983016282320023, 0.32870450615882874, -0.3496316373348236, -0.15630283951759338]
        ]
      >,
      "wig" => #Nx.Tensor<
        f32[1][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72086>
        [
          [0.17028477787971497, 0.01696387305855751, -0.17248104512691498, -0.05966378375887871, -0.12150374799966812, 0.10268787294626236, -0.7275797724723816, -0.09393864125013351, -0.7001956701278687, 0.0028540780767798424, 0.3897332549095154, 0.2860604226589203, 0.3447501063346863, -0.0493205189704895, 0.4567725360393524, 0.58671635389328, -0.06472272425889969, 0.012981667183339596, -0.26912692189216614, 0.7132738828659058, -0.4553998112678528, 0.49699124693870544, 0.45533472299575806, 0.6936178803443909, 0.17375683784484863, 0.1422351896762848, 0.0076703159138560295, 0.4606088399887085, -0.4064836800098419, 0.7237271666526794, -0.04045471176505089, -0.39126941561698914]
        ]
      >,
      "wii" => #Nx.Tensor<
        f32[1][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72087>
        [
          [0.24280501902103424, -0.31628096103668213, -0.2823645770549774, -0.32058680057525635, -0.28626227378845215, 0.4590436816215515, -0.3879033923149109, 0.2936517000198364, 0.011399087496101856, -0.17739838361740112, -0.2046431303024292, 0.007822422310709953, 0.24990378320217133, 0.27935996651649475, -0.10022971779108047, 0.19544894993305206, -0.22647404670715332, 0.5739539265632629, -0.42847177386283875, -0.2944210469722748, 0.41878166794776917, 0.011326911859214306, 0.13541334867477417, -0.30616262555122375, 0.31548038125038147, -0.20294497907161713, -0.049099650233983994, -0.2857513129711151, 0.03543342277407646, 0.22554151713848114, 0.13172614574432373, 0.12445107847452164]
        ]
      >,
      "wio" => #Nx.Tensor<
        f32[1][32]
        EXLA.Backend<cuda:0, 0.1404480797.1338638370.72088>
        [
          [-0.29117146134376526, -0.0981542095541954, -0.3119748532772064, 0.21846914291381836, -0.20995815098285675, -0.04309428110718727, -0.3758567273616791, 0.07012990862131119, -0.05443897843360901, 0.36821800470352173, -0.0019565559923648834, 0.36076390743255615, -0.1976756751537323, 0.37358635663986206, -0.11389780789613724, -0.1219039186835289, 0.2755102217197418, 0.35371896624565125, 0.07545212656259537, 0.24748216569423676, -0.28895965218544006, -0.25804927945137024, 0.34194037318229675, -0.25126567482948303, -0.04256179556250572, 0.013987083919346333, -0.17719265818595886, -0.31035566329956055, 0.44955286383628845, 0.1709146350622177, -0.18998171389102936, 0.508324146270752]
        ]
      >
    }
  },
  "lstm__c_hidden_state" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<cuda:0, 0.1404480797.1338638370.72089>
      [400987821, 3305870042]
    >
  },
  "lstm__h_hidden_state" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<cuda:0, 0.1404480797.1338638370.72090>
      [400987821, 3305870042]
    >
  }
}
```

```elixir
rnn_model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(:mean_absolute_error)
|> Axon.Loop.run(
  single_step_test_data,
  rnn_trained_model_state,
  compiler: EXLA
)
```

<!-- livebook:{"output":true} -->

```

23:09:41.119 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Batch: 14, mean_absolute_error: 0.0027945
```

<!-- livebook:{"output":true} -->

```
%{
  0 => %{
    "mean_absolute_error" => #Nx.Tensor<
      f32
      EXLA.Backend<cuda:0, 0.1404480797.1338638370.75612>
      0.0027944808825850487
    >
  }
}
```

```elixir
0.0032470
|> Kernel.*(:math.sqrt(Explorer.Series.variance(aapl_df["Close"])))
|> Kernel.+(Explorer.Series.mean(aapl_df["Close"]))
```

<!-- livebook:{"output":true} -->

```
64.80750153333416
```

```elixir
Analysis.visualize_predictions(
  rnn_model,
  rnn_trained_model_state,
  Explorer.Series.to_list(normalized_aapl_df["Close"]),
  window_size,
  1,
  batch_size
)
```

## Tempering Expectations

In reality, forecasting the future is really challenging.

Neural networks are really powerful interpolators.

Interpolation is the process of filling in unknown data points within a fixed domain.

Interpolation is explicitly different from extrapolation because in extrapolation we attempt to fill in unknown data points from outside a fixed domain.

You can attempt to extrapolate information about the future from past patterns, but the patterns may change drastically.
