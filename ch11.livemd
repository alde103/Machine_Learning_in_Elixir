<!-- livebook:{"persist_outputs":true} -->

# Chapter 11. Model Everything with Transformers

```elixir
Mix.install(
  [
    {:bumblebee, "~> 0.5"},
    {:axon, "~> 0.6"},
    {:nx, "~> 0.7"},
    {:kino, "~> 0.8"},
    {:kino_bumblebee, ">= 0.0.0"},
    {:exla, ">= 0.0.0"}
  ],
  config: [nx: [default_backend: {EXLA.Backend, client: :cuda}]]
)

# export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
# export XLA_TARGET=cuda120 
# Nx.global_default_backend(EXLA.Backend)
Nx.Defn.global_default_options(compiler: EXLA)
```

## Introduction

Transformers have proven extremely generalizable on a wide-range of input modalities.

Transformers are a class of deep learning models that were originally designed
for natural language processing. they work well in audio processing, computer vision, and more.

There is seemingly no limit to the power of a carefully crafted
transformer model and a lot of input data.

Transformers aren’t necessarily the best model for modeling everything, but
they do quite well at modeling a lot of things.

**Bumblebee** is an Elixir library which consists of pre-trained Axon models as well as out-of-the-box pipelines for solving machine learning problems.

## Paying Attention

The key feature of transformers is in how the apply attention.

Sequence-to-sequence models are designed specifically for problems in which
the objective is to map a source sequence to a target sequence.

Sequence-to-
sequence models consist of an encoder which is responsible for summarizing
the information present in the source sequence in a context vector and a
decoder which is initialized with the context vector and produces the target
sequence.

First, the encoder extracts meaning from the source
sequence, then the decoder uses this meaning to generate a target sequence.

In RNN, long sequences, it’s common for latter portions of the sequence to be overrepresented in the final state, while earlier portions are forgotten.

To overcome the problem of forgetting, you can introduce a shortcut that
computes the alignment between source and target sentences. This is the
attention mechanism.

Attention provides a map that indicates how much each hidden state should be weighed into each output token.

The attention matrix essentially tells you how important each word
in the source sequence is to the target sequence.

Attention enables a model to learn to selectively pay
attention to certain tokens in the input.

## Going from RNNs to Transformers

Attention provides a shortcut between source and target sequences. This
helps a recurrent model learn longer sequences by highlighting which portions
of the context vector map to portions of the target sequence.

The recurrent process of a RNN works on a single time step in the forward or reverse
direction, so it’s difficult to capture relationships between non-adjacent tokens.

Self-attention highlights relationships between words in a sequence.

The typical transformer architecture makes use of an encoder-decoder architecture on a masked language modeling task.

The encoder in a transformer typically consists of a collection of multiple
encoder layers and operates on a representation of the source sequence. Each
encoder layer is identical, and consists of some form of self-attention and
some series of output projections. The actual variant of self-attention in use
in a transformer is typically multi-head self-attention. Multi-head self-attention
is identical to self-attention with one slight exception.

In single-headed self-attention, the model computes one attention matrix,
which means it learns to weight the relationships between tokens in one way.
In multi-headed self-attention, the model computes multiple (typically 12)
different attention matrices (heads).

The decoder in a transformer model typically consists of a collection of multiple
decoder layers and operates on a representation of the target
sequence—usually the source sequence shifted to the right by one token—and
some information about the source sequence from the encoder. Each decoder
layer also consists of some variant of self-attention, but the decoder also
makes use of cross-attention. The decoder layer computes multi-headed self-
attention on the target sequence. The decoder layer also computes multi-
headed cross-attention between information provided by the encoder and the
target sequence.

One issue with using just attention to model temporal data is that transformers have no way of representing temporal dependencies. Most transformers introduce a positional embedding or positional encoding which injects information about positional relationships into the
model.

Most models are differentiated only by
slight tweaks in specific parts such as the attention implementation, the
positional encoding used, or the training process.

Transformers prove significantly more powerful than their recurrent counterparts. Additionally, transformers are readily parallelizable and are significantly more scalable
than an equivalent recurrent neural network.

```elixir

```
