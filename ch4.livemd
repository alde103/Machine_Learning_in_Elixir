<!-- livebook:{"persist_outputs":true} -->

# Chapter 4. Optimize Everything

```elixir
Mix.install([
{:nx, "~> 0.5"}
])
```

## Learning with Optimization

A machine learning system as any computing system that is capable of improving from experience on a specific task with respect to some arbitrary performance measure.

The end goal is to have a system capable of providing predictions about unseen examples.

Optimization is the search for the best. A machine learning model may look something like this:

```elixir
# def predict(input) do
#  label = f(input, params)
#  label
# end

# def loss(actual_labels, predicted_labels) do
#   loss_value = measure_difference(actual_labels, predicted_labels)
#   loss_value
# end

# def objective(params, actual_inputs, actual_labels) do
#   predicted_labels = model(params, actual_inputs)
#   loss(actual_labels, predicted_labels)
# end
```

<!-- livebook:{"output":true} -->

```
nil
```

Here, f represents the transformation performed by your machine learning
algorithm of choice, and params represents the learned parameters for your
algorithm. The goal is to find the parameters that best map inputs to labels
with the function. Ideally, you want params for f that yield the best performance on the task at hand. You want to optimize params to achieve the best performance on your task.

During training you define a loss function, or, cost function, that you then
attempt to optimize with respect to your model’s parameters.

## Defining Objectives

The definition of best parameters for a model is subject to the constraints and objectives put forth by you when defining your machine learning task.

Surrogate loss functions serve as proxies for true objectives, optimizing
them typically also indirectly optimizes the performance objectives you care
about. For example, it’s much more common to use squared error as a surrogate for absolute error because the squared error function has properties that
make optimization easier.

Likelihood Estimation

In statistics, the likelihood function (a.k.a. likelihood) describes the probability of observed data as a function of its parameters. Think of likelihood as a sort of truth meter. Statistical learning theory assumes there exists some true function that generates data.

```elixir
# Cross-entropy

defn binary_cross_entropy(y_true, y_pred) do
  y_true * Nx.log(y_pred) - (1 - y_true) * Nx.log(1 - y_pred)
end
```

<!-- livebook:{"output":true} -->

```
error: undefined function defn/2 (there is no such import)
└─ /home/alde/Documents/MyDevelopment/Machine_Learning_in_Elixir/ch4.livemd#cell:rj6seumisbkcvknc:3

```

```elixir
# Mean Squared Error

# Mean squared error measures per-example loss as the average squared differ-
# ence between true labels and predicted labels.

defn mean_squared_error(y_true, y_pred) do
  y_true
  |> Nx.subtract(y_pred)
  |> Nx.pow(2)
  |> Nx.mean(axes: [-1])
end

# Absolute error
defn absolute_error(y_true, y_pred) do
  y_true
  |> Nx.subtract(y_pred)
  |> Nx.abs(2)
end
```

<!-- livebook:{"output":true} -->

```
error: undefined function defn/2 (there is no such import)
└─ /home/alde/Documents/MyDevelopment/Machine_Learning_in_Elixir/ch4.livemd#cell:lu5ndcwkatzhdxmi:6

```

## Converging to a Solution

Most of the optimization algorithms used in machine learning don’t
have any guarantees of optimality.

The best set of model parameters are the global optima of the entire parameter
space. With infinite parameter spaces and functions that cannot be optimized
analytically

Finding the global optima is impossible. Instead, optimization routines are mechanical and designed to loop until some desired performance threshold.

Local optima are localized regions of a parameter space that are better than neighboring points.

## Regularizing to Generalize

The difference between machine learning and optimization is that machine learning is concerned with performance on unseen data, this concept is known as generalization.

## Overfitting, Underfitting, and Capacity

**Overfitting** is a scenario in which a trained model has low training error but high generalization error.

**Underfitting** is a scenario in which a model doesn’t even have low training error.

Both overfitting and underfitting are typically functions of a model’s capacity.

A model’s capacity is its ability to fit many different functions.

Models with higher capacity are more prone to overfitting than models with
lower capacity because models with high capacity can fit a wider range of
complex functions.

Designing good models means finding the balance between scenarios
of overfitting and underfitting.

## Defining Regularization

Regularization is any technique used to reduce generalization error.

**Complexity Penalities**

Complexity penalties impose a cost at model evaluation time by adding a penalty term with some penalty weight. For example weight decvay using the L2-Norm to penalize the parameters that are far from the origin, e.g. Weight-decay expresses the preference for smaller model weights.

**Early-stopping**

Early-stopping is when the training process is stopped if overfitting is detected.

Validation sets or holdout sets are portions of the original training data that are not used to train but instead are used to periodically monitor model performance.

If training errors continue to decrease, but validation errors start to increase, the model is starting to overfit.

## Descending Gradients

```elixir
# The gradients of a scalar function are indicative of the direction 
# of steepest descent, so they can be useful in determining how to 
# navigate a function in order to find the minimum.

# Generate train and test data
key = Nx.Random.key(42)
{true_params, new_key} = Nx.Random.uniform(key, shape: {32, 1})

true_function = fn params, x ->
  Nx.dot(x, params)
end

{train_x, new_key} = Nx.Random.uniform(new_key, shape: {10000, 32})
train_y = true_function.(true_params, train_x)
train_data = Enum.zip(Nx.to_batched(train_x, 1), Nx.to_batched(train_y, 1))
{test_x, _new_key} = Nx.Random.uniform(new_key, shape: {10000, 32})
test_y = true_function.(true_params, test_x)
test_data = Enum.zip(Nx.to_batched(test_x, 1), Nx.to_batched(test_y, 1))
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.9657397270202637, 0.9266661405563354, 0.2524207830429077, 0.506806492805481, 0.03272294998168945, 0.6381621360778809, 0.4016733169555664, 0.4144333600997925, 0.8692346811294556, 0.19583988189697266, 0.4356701374053955, 0.037007689476013184, 0.4367654323577881, 0.9086041450500488, 0.4730778932571411, 0.29556596279144287, 0.49315857887268066, 0.3683987855911255, 0.8670364618301392, 0.527277946472168, 0.028360843658447266, 0.13743293285369873, 0.8709059953689575, 0.1861327886581421, 0.4181276559829712, 0.9427480697631836, 0.4339343309402466, 0.8707499504089355, 0.6826666593551636, 0.528895378112793, 0.17522680759429932, 0.4048128128051758]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.652977466583252]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.4392540454864502, 0.9165053367614746, 0.9777518510818481, 0.879123330116272, 0.612689733505249, 0.01696908473968506, 0.133436918258667, 0.4318392276763916, 0.5053318738937378, 0.7980244159698486, 0.1885296106338501, 0.9951480627059937, 0.3975728750228882, 0.226912260055542, 0.4825739860534668, 0.9671891927719116, 0.24038493633270264, 0.13231432437896729, 0.38793301582336426, 0.05815780162811279, 0.43374860286712646, 0.2860398292541504, 0.6426401138305664, 0.8966696262359619, 0.09666109085083008, 0.4394463300704956, 0.35843217372894287, 0.34688258171081543, 0.5460761785507202, 0.5041118860244751, 0.5477373600006104, 0.8824354410171509]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.90969705581665]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.7762134075164795, 0.7822309732437134, 0.24949312210083008, 0.24509012699127197, 0.9004219770431519, 0.8151938915252686, 0.9005923271179199, 0.8640304803848267, 0.4731714725494385, 0.5921633243560791, 0.4887489080429077, 0.8375271558761597, 0.9577419757843018, 0.5891522169113159, 0.12607717514038086, 0.708527684211731, 0.41328203678131104, 0.6296629905700684, 0.6268273591995239, 0.35883355140686035, 0.36125707626342773, 0.6910197734832764, 0.7902359962463379, 0.7439805269241333, 0.4775749444961548, 0.9078165292739868, 0.3568282127380371, 0.15519630908966064, 0.11200845241546631, 0.7795575857162476, 0.468631386756897, 0.9759647846221924]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [8.54786205291748]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.3197380304336548, 0.17616689205169678, 0.8258638381958008, 0.3432091474533081, 0.35468900203704834, 0.5186667442321777, 0.7499172687530518, 0.4087836742401123, 0.4280334711074829, 0.2278900146484375, 0.6885714530944824, 0.6648629903793335, 0.5448073148727417, 0.1430720090866089, 0.842303991317749, 0.8900002241134644, 0.4492759704589844, 0.8455078601837158, 0.4587341547012329, 0.3691824674606323, 0.2542390823364258, 0.871134877204895, 0.26322853565216064, 0.10538768768310547, 0.355352520942688, 0.8888055086135864, 0.488552451133728, 0.6250888109207153, 0.9855941534042358, 0.738310694694519, 0.6712085008621216, 0.04661083221435547]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [8.220166206359863]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.40412354469299316, 0.7216566801071167, 0.25867438316345215, 0.9800392389297485, 0.9075496196746826, 0.6819312572479248, 0.0149993896484375, 0.04356968402862549, 0.9605370759963989, 0.02644956111907959, 0.28210699558258057, 0.7568575143814087, 0.21303772926330566, 0.002684950828552246, 0.6519417762756348, 0.28664088249206543, 0.15737569332122803, 0.37507736682891846, 0.05415797233581543, 0.03802788257598877, 0.8071837425231934, 0.06110048294067383, 0.6388435363769531, 0.44481122493743896, 0.23555970191955566, 0.61528480052948, 0.8113986253738403, 0.012137651443481445, 0.9276052713394165, 0.9450554847717285, 0.9840184450149536, 0.20820486545562744]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.5124430656433105]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.5034708976745605, 0.7755038738250732, 0.13867413997650146, 0.29906952381134033, 0.014742374420166016, 0.7755328416824341, 0.9173959493637085, 0.0935053825378418, 0.31686699390411377, 0.06115245819091797, 0.8989229202270508, 0.19432556629180908, 0.7501810789108276, 0.2113250494003296, 0.5822068452835083, 0.6005638837814331, 0.625515341758728, 0.6752986907958984, 0.9507982730865479, 0.7879356145858765, 0.5397478342056274, 0.3113539218902588, 0.8102543354034424, 0.2979027032852173, 0.7655726671218872, 0.42514193058013916, 0.09351170063018799, 0.8037655353546143, 0.4778313636779785, 0.44777703285217285, 0.3096567392349243, 0.33784306049346924]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.7442731857299805]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.7518961429595947, 0.22414886951446533, 0.15714240074157715, 0.8663241863250732, 0.508256196975708, 0.30795371532440186, 0.11998486518859863, 0.18344223499298096, 0.4011112451553345, 0.924648642539978, 0.5058850049972534, 0.5193443298339844, 0.9716345071792603, 0.948397159576416, 0.5351895093917847, 0.5134536027908325, 0.6595708131790161, 0.06837213039398193, 0.05189085006713867, 0.8435298204421997, 0.7968239784240723, 0.12332558631896973, 0.7250438928604126, 0.7147141695022583, 0.8842874765396118, 0.9462226629257202, 0.843963623046875, 0.8965095281600952, 0.4305756092071533, 0.5930991172790527, 0.11764276027679443, 0.5833710432052612]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.623931884765625]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.9475998878479004, 0.6403565406799316, 0.5817118883132935, 0.5467712879180908, 0.8543612957000732, 0.06530475616455078, 0.14756488800048828, 0.15206146240234375, 0.68489670753479, 0.932651162147522, 0.9179636240005493, 0.8796118497848511, 0.2882128953933716, 0.7526837587356567, 0.1426788568496704, 0.18050217628479004, 0.6951268911361694, 0.7308511734008789, 0.6911174058914185, 0.19187331199645996, 0.925081729888916, 0.8188349008560181, 0.5788781642913818, 0.33968937397003174, 0.8412926197052002, 0.50633704662323, 0.40607786178588867, 0.39345502853393555, 0.9535032510757446, 0.0635685920715332, 0.7170870304107666, 0.8757264614105225]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [8.50348949432373]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.01942753791809082, 0.899272084236145, 0.5304620265960693, 0.5620572566986084, 0.128362774848938, 0.31026554107666016, 0.6900253295898438, 0.2783416509628296, 0.004452347755432129, 0.5778182744979858, 0.026953697204589844, 0.014599919319152832, 0.3131972551345825, 0.6139227151870728, 0.6718645095825195, 0.9182217121124268, 0.4055975675582886, 0.9959360361099243, 0.3222285509109497, 0.1344226598739624, 0.8531616926193237, 0.1252962350845337, 0.7893067598342896, 0.6823188066482544, 0.38434433937072754, 0.0016857385635375977, 0.9079246520996094, 0.33411502838134766, 0.05022597312927246, 0.5846171379089355, 0.889033317565918, 0.7293587923049927]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.110525608062744]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.8360906839370728, 0.4355432987213135, 0.11632812023162842, 0.7702548503875732, 0.24256396293640137, 0.36099421977996826, 0.6917792558670044, 0.288962721824646, 0.6243956089019775, 0.9861946105957031, 0.3847709894180298, 0.6880143880844116, 0.589323878288269, 0.4923354387283325, 0.3279656171798706, 0.4151395559310913, 0.852401614189148, 0.0718458890914917, 0.01529836654663086, 0.06954300403594971, 0.7971522808074951, 0.7249754667282104, 0.25757861137390137, 0.906819224357605, 0.6608389616012573, 0.40988433361053467, 0.26649951934814453, 0.6497167348861694, 0.31986987590789795, 0.8541487455368042, 0.7966134548187256, 0.23020529747009277]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.6582722663879395]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.7711091041564941, 0.6362074613571167, 0.8661282062530518, 0.6193832159042358, 0.6161632537841797, 0.19212019443511963, 0.7516170740127563, 0.023564815521240234, 0.7833913564682007, 0.8175530433654785, 0.029859185218811035, 0.30578577518463135, 0.9423370361328125, 0.20194673538208008, 0.6542264223098755, 0.9779584407806396, 0.11775898933410645, 0.5317223072052002, 0.3922593593597412, 0.832879900932312, 0.657945990562439, 0.43512094020843506, 0.32924580574035645, 0.21120929718017578, 0.76695716381073, 0.9446995258331299, 0.02226400375366211, 0.8510793447494507, 0.06922507286071777, 0.03070998191833496, 0.9929032325744629, 0.6356418132781982]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.539000988006592]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.5236676931381226, 0.2989940643310547, 0.03134489059448242, 0.12583446502685547, 0.042815566062927246, 0.8364819288253784, 0.31674015522003174, 0.04060947895050049, 0.05521893501281738, 0.7784453630447388, 0.48672759532928467, 0.8186780214309692, 0.00807809829711914, 0.39795219898223877, 0.22978472709655762, 0.5791110992431641, 0.6117820739746094, 0.7412447929382324, 0.42317402362823486, 0.28765225410461426, 0.36166059970855713, 0.5173482894897461, 0.9059319496154785, 0.3208935260772705, 0.3955960273742676, 0.5770881175994873, 0.963921308517456, 0.05305802822113037, 0.009126543998718262, 0.30502188205718994, 0.348180890083313, 0.28527331352233887]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [5.539951324462891]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.7139891386032104, 0.12666535377502441, 0.15333807468414307, 0.7544382810592651, 0.9695196151733398, 0.49136245250701904, 0.20070266723632812, 0.005652427673339844, 0.02549123764038086, 0.3883492946624756, 0.7958550453186035, 0.4632751941680908, 0.14279115200042725, 0.5663028955459595, 0.31234872341156006, 0.6877082586288452, 0.04052889347076416, 0.19631445407867432, 0.8272514343261719, 0.7589792013168335, 0.8727586269378662, 0.9460961818695068, 0.7840994596481323, 0.1846456527709961, 0.7626980543136597, 0.5093346834182739, 0.5205307006835938, 0.2435612678527832, 0.4535341262817383, 0.3754945993423462, 0.9493304491043091, 0.05621170997619629]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.268062591552734]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.8532519340515137, 0.6970062255859375, 0.652370810508728, 0.4061359167098999, 0.11044573783874512, 0.11151957511901855, 0.851331353187561, 0.6565314531326294, 0.33859121799468994, 0.7652009725570679, 0.3588383197784424, 0.07348513603210449, 0.7815285921096802, 0.9533407688140869, 0.8006638288497925, 0.04949069023132324, 0.8293572664260864, 0.3746068477630615, 0.8676903247833252, 0.9169406890869141, 0.9336735010147095, 0.06596994400024414, 0.8225301504135132, 0.18987727165222168, 0.24470460414886475, 0.8587253093719482, 0.8066114187240601, 0.4743626117706299, 0.8888722658157349, 0.36300718784332275, 0.2819058895111084, 0.5664075613021851]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.505477428436279]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.3902559280395508, 0.14534735679626465, 0.09916174411773682, 0.7248067855834961, 0.20137739181518555, 0.6646915674209595, 0.0778660774230957, 0.8685482740402222, 0.641196608543396, 0.5231763124465942, 0.6376198530197144, 0.6526670455932617, 0.2163105010986328, 0.8063833713531494, 0.6756443977355957, 0.4447154998779297, 0.20969760417938232, 0.951002836227417, 0.045929908752441406, 0.17532849311828613, 0.9260181188583374, 0.5131326913833618, 0.30024540424346924, 0.3300440311431885, 0.9004764556884766, 0.8441228866577148, 0.9477831125259399, 0.6751878261566162, 0.4996030330657959, 0.2638866901397705, 0.7265427112579346, 0.49843263626098633]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.501368045806885]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.8431416749954224, 0.20925045013427734, 0.882979154586792, 0.7653672695159912, 0.8408812284469604, 0.9956172704696655, 0.039161086082458496, 0.2890273332595825, 0.8148702383041382, 0.7094781398773193, 0.45367276668548584, 0.49341559410095215, 0.40397489070892334, 0.044689178466796875, 0.2746458053588867, 0.14785456657409668, 0.5764540433883667, 0.9384440183639526, 0.16802644729614258, 0.5668824911117554, 0.7575173377990723, 0.9617680311203003, 0.34545373916625977, 0.9809876680374146, 0.9966757297515869, 0.9557144641876221, 0.9793694019317627, 0.49138343334198, 0.327367901802063, 0.8423446416854858, 0.41049087047576904, 0.16183257102966309]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.983162879943848]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.22011446952819824, 0.34421753883361816, 0.6294939517974854, 0.5911561250686646, 0.3088088035583496, 0.23263812065124512, 0.3219066858291626, 0.03805994987487793, 0.1934577226638794, 0.9446452856063843, 0.9956920146942139, 0.5987362861633301, 0.26041269302368164, 0.8809354305267334, 0.768547534942627, 0.8348363637924194, 0.826107382774353, 0.6921907663345337, 0.046318769454956055, 0.5803121328353882, 0.9755550622940063, 0.917121410369873, 0.30008530616760254, 0.2571007013320923, 0.6167714595794678, 0.022228240966796875, 0.7143242359161377, 0.5102230310440063, 0.16165518760681152, 0.9506291151046753, 0.9326122999191284, 0.2996530532836914]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.267423152923584]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.38776493072509766, 0.7318899631500244, 0.1307905912399292, 0.09548139572143555, 0.8834151029586792, 0.25518178939819336, 0.8047440052032471, 0.5709458589553833, 0.022022604942321777, 0.8857442140579224, 0.802797794342041, 0.5964082479476929, 0.6515809297561646, 0.06845474243164062, 0.37185752391815186, 0.12856698036193848, 0.09193015098571777, 0.738864541053772, 0.43469250202178955, 0.7443429231643677, 0.016843795776367188, 0.13896071910858154, 0.9344608783721924, 0.04187476634979248, 0.021153926849365234, 0.5739061832427979, 0.11942613124847412, 0.6132626533508301, 0.5382595062255859, 0.9019054174423218, 0.3720097541809082, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.044473648071289]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.5642416477203369, 0.830094575881958, 0.11100232601165771, 0.38808906078338623, 0.2167491912841797, 0.9876217842102051, 0.8534290790557861, 0.38251447677612305, 0.9997782707214355, 0.5055009126663208, 0.836064338684082, 0.26873278617858887, 0.44322919845581055, 0.9741466045379639, 0.9353281259536743, 0.6532653570175171, 0.21104705333709717, 0.9035766124725342, 0.11548709869384766, 0.793843150138855, 0.591930627822876, 0.41485321521759033, 0.41184914112091064, 0.5477373600006104, 0.08824586868286133, 0.7526575326919556, 0.44268131256103516, 0.0763329267501831, 0.4934626817703247, 0.8778635263442993, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.992553234100342]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.8263260126113892, 0.47554445266723633, 0.7326714992523193, 0.4069685935974121, 0.48123836517333984, 0.9155631065368652, 0.19115734100341797, 0.8787575960159302, 0.5277758836746216, 0.08595383167266846, 0.6884660720825195, 0.3847067356109619, 0.4597644805908203, 0.11427831649780273, 0.611096978187561, 0.17932391166687012, 0.046775102615356445, 0.15377891063690186, 0.17494094371795654, 0.6756585836410522, 0.6878424882888794, 0.23508989810943604, 0.795313835144043, 0.28050291538238525, 0.006268501281738281, 0.423947811126709, 0.3076256513595581, 0.712846040725708, 0.33599305152893066, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.1456146240234375]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.30186140537261963, 0.9873268604278564, 0.035573720932006836, 0.4155128002166748, 0.24097764492034912, 0.03020632266998291, 0.20567595958709717, 0.5887387990951538, 0.6643663644790649, 0.31537091732025146, 0.9205235242843628, 0.6104476451873779, 0.9955638647079468, 0.08851909637451172, 0.34864628314971924, 0.5111007690429688, 0.8216805458068848, 0.9719328880310059, 0.7817020416259766, 0.5537395477294922, 0.42271876335144043, 0.07025539875030518, 0.20106756687164307, 0.06972110271453857, 0.26150715351104736, 0.41637277603149414, 0.36588919162750244, 0.3534187078475952, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.348757266998291]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.8590803146362305, 0.5636343955993652, 0.03443634510040283, 0.5104682445526123, 0.606324315071106, 0.9271607398986816, 0.4498816728591919, 0.8567771911621094, 0.0051795244216918945, 0.3874478340148926, 0.1921311616897583, 0.6639413833618164, 0.46572935581207275, 0.8383623361587524, 0.9469438791275024, 0.020708560943603516, 0.15898573398590088, 0.944486141204834, 0.3688013553619385, 0.3565635681152344, 0.4661533832550049, 0.28654932975769043, 0.9991466999053955, 0.395352840423584, 0.1945810317993164, 0.37531542778015137, 0.34606099128723145, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.0145039558410645]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.40698301792144775, 0.39485931396484375, 0.7665799856185913, 0.45240330696105957, 0.3180277347564697, 0.6046972274780273, 0.27099859714508057, 0.32419121265411377, 0.15665650367736816, 0.8422553539276123, 0.17924022674560547, 0.9220238924026489, 0.6719664335250854, 0.3106119632720947, 0.7598171234130859, 0.41273176670074463, 0.24574947357177734, 0.4455568790435791, 0.7285884618759155, 0.5802567005157471, 0.7816479206085205, 0.43250608444213867, 0.7760515213012695, 0.9805769920349121, 0.48999035358428955, 0.6657071113586426, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.522810935974121]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.6499532461166382, 0.33332526683807373, 0.18239235877990723, 0.28092002868652344, 0.6100766658782959, 0.8831055164337158, 0.5425313711166382, 0.46851158142089844, 0.0425875186920166, 0.4390711784362793, 0.24202609062194824, 0.4561119079589844, 0.30997657775878906, 0.8286688327789307, 0.9777672290802002, 0.7183065414428711, 0.994769811630249, 0.9684973955154419, 0.516169548034668, 0.7496813535690308, 0.1341181993484497, 0.23441553115844727, 0.4797624349594116, 0.7527389526367188, 0.706782341003418, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.673696041107178]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.29753637313842773, 0.9054058790206909, 0.5384794473648071, 0.008930683135986328, 0.9904316663742065, 0.012788891792297363, 0.6981593370437622, 0.21004319190979004, 0.24448156356811523, 0.3577829599380493, 0.8826776742935181, 0.556861162185669, 0.7953556776046753, 0.755801796913147, 0.8585830926895142, 0.8141891956329346, 0.611968994140625, 0.02731025218963623, 0.8326984643936157, 0.026828527450561523, 0.04161202907562256, 0.72672438621521, 0.16502487659454346, 0.3540531396865845, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.771440029144287]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.22602331638336182, 0.938198447227478, 0.6433297395706177, 0.22689640522003174, 0.2643556594848633, 0.38158679008483887, 0.18027138710021973, 0.7762355804443359, 0.8237777948379517, 0.16852951049804688, 0.2584739923477173, 0.38265061378479004, 0.028604865074157715, 0.05207550525665283, 0.08737969398498535, 0.831161379814148, 0.337926983833313, 0.578020453453064, 0.1244196891784668, 0.3897353410720825, 0.6503366231918335, 0.5011011362075806, 0.3202742338180542, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.270139217376709]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.1591346263885498, 0.3551875352859497, 0.19456207752227783, 0.08360016345977783, 0.49338340759277344, 0.8102091550827026, 0.038229942321777344, 0.5133059024810791, 0.42447197437286377, 0.1302204132080078, 0.032562255859375, 0.4770599603652954, 0.10980844497680664, 0.39250481128692627, 0.9826400279998779, 0.6408740282058716, 0.4889005422592163, 0.8684313297271729, 0.6371150016784668, 0.5141459703445435, 0.04483532905578613, 0.008337259292602539, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.415218830108643]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.6458258628845215, 0.14830482006072998, 0.1367412805557251, 0.05629110336303711, 0.6188018321990967, 0.09611082077026367, 0.022463202476501465, 0.16802668571472168, 0.13714361190795898, 0.9738653898239136, 0.22950482368469238, 0.14387571811676025, 0.3699824810028076, 0.037640929222106934, 0.8094890117645264, 0.16305005550384521, 0.9263873100280762, 0.4807380437850952, 0.35078299045562744, 0.5584464073181152, 0.42825543880462646, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.667717456817627]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.6350167989730835, 0.7023348808288574, 0.9816912412643433, 0.3239823579788208, 0.3320831060409546, 0.9436564445495605, 0.8940634727478027, 0.9500092267990112, 0.039070963859558105, 0.7722084522247314, 0.014232039451599121, 0.5869686603546143, 0.6310857534408569, 0.754490852355957, 0.4464530944824219, 0.28466737270355225, 0.12341678142547607, 0.28795409202575684, 0.03844261169433594, 0.05093085765838623, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.96828556060791]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.1708146333694458, 0.7656099796295166, 0.17411589622497559, 0.13566994667053223, 0.5877799987792969, 0.8375746011734009, 0.7500095367431641, 0.43630826473236084, 0.6984328031539917, 0.458881139755249, 0.3605443239212036, 0.6612114906311035, 0.7714365720748901, 0.18256628513336182, 0.6045645475387573, 0.6824719905853271, 0.093025803565979, 0.8150986433029175, 0.6484025716781616, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.744658946990967]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.37816083431243896, 0.3764007091522217, 0.366793155670166, 0.5291237831115723, 0.4347909688949585, 0.051524996757507324, 0.7034255266189575, 0.21059012413024902, 0.4240748882293701, 0.33774685859680176, 0.602317214012146, 0.8616265058517456, 0.09764957427978516, 0.8331103324890137, 0.6966776847839355, 0.9676048755645752, 0.39181971549987793, 0.5993291139602661, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.194350242614746]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.9014437198638916, 0.6692262887954712, 0.2199995517730713, 0.34047043323516846, 0.5105847120285034, 0.5800155401229858, 0.6711152791976929, 0.24664950370788574, 0.837967038154602, 0.1854724884033203, 0.4090847969055176, 0.5367509126663208, 0.9298523664474487, 0.8082610368728638, 0.4719582796096802, 0.9887839555740356, 0.8493902683258057, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.900388240814209]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.18410146236419678, 0.16370105743408203, 0.00571596622467041, 0.6846107244491577, 0.8010516166687012, 0.5752760171890259, 0.7778260707855225, 0.570926308631897, 0.2175813913345337, 0.15920031070709229, 0.7178256511688232, 0.7729295492172241, 0.09782063961029053, 0.5135018825531006, 0.6833776235580444, 0.07111799716949463, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.223117351531982]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.9505445957183838, 0.6332504749298096, 0.4598352909088135, 0.821086049079895, 0.4521135091781616, 0.40192127227783203, 0.37758028507232666, 0.7068672180175781, 0.5899826288223267, 0.3339945077896118, 0.5181410312652588, 0.5538294315338135, 0.32791435718536377, 0.6153789758682251, 0.21849405765533447, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.009317874908447]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.5818334817886353, 0.008952617645263672, 0.817952036857605, 0.8802107572555542, 0.2041914463043213, 0.6401019096374512, 0.5620921850204468, 0.6981043815612793, 0.9288793802261353, 0.17047274112701416, 0.8074302673339844, 0.027227401733398438, 0.8451176881790161, 0.8029766082763672, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.590928554534912]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.21364152431488037, 0.9299622774124146, 0.176169753074646, 0.40923750400543213, 0.0010215044021606445, 0.6993589401245117, 0.2511633634567261, 0.6728019714355469, 0.2957075834274292, 0.5348055362701416, 0.8762129545211792, 0.07801520824432373, 0.3015238046646118, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.7690606117248535]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.3406723737716675, 0.8631099462509155, 0.12643325328826904, 0.38934123516082764, 0.46326422691345215, 0.039055824279785156, 0.5590641498565674, 0.24887871742248535, 0.38112592697143555, 0.7917823791503906, 0.8130742311477661, 0.016570329666137695, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [8.340514183044434]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.05386042594909668, 0.0680922269821167, 0.31601762771606445, 0.18086862564086914, 0.7679719924926758, 0.6589527130126953, 0.9141957759857178, 0.402393102645874, 0.8808540105819702, 0.9081192016601562, 0.6332200765609741, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.193706512451172]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.7115259170532227, 0.19793486595153809, 0.3831000328063965, 0.12318956851959229, 0.048275113105773926, 0.6922358274459839, 0.8630118370056152, 0.6173487901687622, 0.4392777681350708, 0.7511276006698608, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [5.914463043212891]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.15324389934539795, 0.608772873878479, 0.30232787132263184, 0.42286384105682373, 0.6527767181396484, 0.8560984134674072, 0.95783531665802, 0.35850799083709717, 0.20661818981170654, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.008178234100342]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.9347794055938721, 0.5582127571105957, 0.8688193559646606, 0.9338347911834717, 0.6681429147720337, 0.09503507614135742, 0.1364145278930664, 0.2338886260986328, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.377873420715332]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.44904494285583496, 0.6156696081161499, 0.24088788032531738, 0.04646635055541992, 0.1615074872970581, 0.3069014549255371, 0.476338267326355, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [5.039738178253174]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.26967108249664307, 0.34533655643463135, 0.32481229305267334, 0.032245635986328125, 0.33962345123291016, 0.5251162052154541, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [7.07838249206543]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.16084957122802734, 0.4878326654434204, 0.31659018993377686, 0.9245070219039917, 0.24317359924316406, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.446048259735107]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.2458251714706421, 0.5735921859741211, 0.2629578113555908, 0.3110496997833252, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [5.8523969650268555]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.7443956136703491, 0.07421326637268066, 0.4091228246688843, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [8.270795822143555]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.6418168544769287, 0.5278061628341675, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       [6.982004642486572]
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       [0.5448164939880371, ...]
     ]
   >,
   #Nx.Tensor<
     f32[1][1]
     [
       ...
     ]
   >},
  {#Nx.Tensor<
     f32[1][32]
     [
       ...
     ]
   >, ...},
  {...},
  ...
]
```

## Getting a Good Initialization

```elixir
defmodule SGD do
  import Nx.Defn

  # Stochastic gradient descent typically starts from 
  # a random set of parameters.
  defn init_random_params(key) do
    Nx.Random.uniform(key, shape: {32, 1})
  end

  defn model(params, inputs) do
    labels = Nx.dot(inputs, params)
    labels
  end

  defn mean_squared_error(y_true, y_pred) do
    y_true
    |> Nx.subtract(y_pred)
    |> Nx.pow(2)
    |> Nx.mean(axes: [-1])
  end

  # The loss function gives  a measure of how close the predicted 
  # distribution is to the real distribution.
  defn loss(actual_label, predicted_label) do
    loss_value = mean_squared_error(actual_label, predicted_label)
    loss_value
  end

  defn objective(params, actual_inputs, actual_labels) do
    predicted_labels = model(params, actual_inputs)
    loss(actual_labels, predicted_labels)
  end

  # The learning rate (1e-2). It essentially controls the size of jumps or
  # changes between parameters for each example.
  defn step(params, actual_inputs, actual_labels) do
    {loss, params_grad} =
      value_and_grad(params, fn params ->
        objective(params, actual_inputs, actual_labels)
      end)

    new_params = params - 1.0e-2 * params_grad
    {loss, new_params}
  end

  def evaluate(trained_params, test_data) do
    test_data
    |> Enum.map(fn {x, y} ->
      prediction = model(trained_params, x)
      loss(y, prediction)
    end)
    |> Enum.reduce(0, &Nx.add/2)
  end

  def train(data, iterations, key) do
    {params, _key} = init_random_params(key)
    loss = Nx.tensor(0.0)

    {_, trained_params} =
      for i <- 1..iterations, reduce: {loss, params} do
        {loss, params} ->
          for {{x, y}, j} <- Enum.with_index(data), reduce: {loss, params} do
            {loss, params} ->
              {batch_loss, new_params} = step(params, x, y)
              avg_loss = Nx.add(Nx.mean(batch_loss), loss) |> Nx.divide(j + 1)
              IO.write("\rEpoch: #{i}, Loss: #{Nx.to_number(avg_loss)}")
              {avg_loss, new_params}
          end
      end

    trained_params
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, SGD, <<70, 79, 82, 49, 0, 0, 31, ...>>, {:train, 3}}
```

## Evaluating the Algorithm

```elixir
key = Nx.Random.key(10000)
trained_params = SGD.train(train_data, 1, key)
```

<!-- livebook:{"output":true} -->

```
Epoch: 1, Loss: 9.09517442807617e-17
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[32][1]
  [
    [0.29071536660194397],
    [0.07259715348482132],
    [0.21026252210140228],
    [0.5500629544258118],
    [0.7445037961006165],
    [0.8719053864479065],
    [0.3432203233242035],
    [0.1635684072971344],
    [0.007242138963192701],
    [0.09632112085819244],
    [0.8762232661247253],
    [0.31724998354911804],
    [0.5961995720863342],
    [0.0737060084939003],
    [0.7349367141723633],
    [0.8990370035171509],
    [0.6406899094581604],
    [0.5622185468673706],
    [0.6447593569755554],
    [0.005062112119048834],
    [0.7313767671585083],
    [0.6215839982032776],
    [0.2681311070919037],
    [0.3174809515476227],
    [0.19240215420722961],
    [0.7703497409820557],
    [0.019297830760478973],
    [0.5594630241394043],
    [0.9107012748718262],
    [0.053947582840919495],
    [0.20060569047927856],
    [0.8796132802963257]
  ]
>
```

```elixir
key = Nx.Random.key(100)
{random_params, _} = SGD.init_random_params(key)
SGD.evaluate(random_params, test_data)
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1]
  [4059.184814453125]
>
```

## Making it fail

```elixir
key = Nx.Random.key(42)
{true_params, new_key} = Nx.Random.uniform(key, shape: {32, 1})

true_function = fn params, x ->
  Nx.dot(x, params) |> Nx.cos()
end

{train_x, new_key} = Nx.Random.uniform(new_key, shape: {10000, 32})
train_y = true_function.(true_params, train_x)
train_data = Enum.zip(Nx.to_batched(train_x, 1), Nx.to_batched(train_y, 1))
{test_x, _new_key} = Nx.Random.uniform(new_key, shape: {10000, 32})
test_y = true_function.(true_params, test_x)
test_data = Enum.zip(Nx.to_batched(test_x, 1), Nx.to_batched(test_y, 1))

key = Nx.Random.key(0)
trained_params = SGD.train(train_data, 10, key)
SGD.evaluate(trained_params, test_data)
```

<!-- livebook:{"output":true} -->

```
Epoch: 10, Loss: 4.419049218995497e-6
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[1]
  [2648.607666015625]
>
```

## Peering into the Black-box

Hyperparameters are details about an algorithm that are not directly learnable but affect the structure and outcome of the model training process.

Hyperparameter search is concerned with finding hyperparameters which
lead to optimal performance of a model on an evaluation set.

The goal in hyperparameter search is typically to maximize validation metrics.

Hyperparameter search often relies on black-box optimization and exhaustive search. Black-box optimization optimize solutions based on objective observations without needing access to an explicit objective function. An exhaustive search is just a brute-force search.

**Evolutionary Algorithms**

Evolutionary algorithms are a class of optimization algorithms based on the
principles of artificial selection.

In an evolutionary algorithm, you generate a population of solutions to a problem (e.g., a set of hyperparameters), evaluate each solution in the population, and then combine the best solutions to form better solutions.

Evolutionary algorithms can even be used in tandem with neural networks in a process called neuro-evolution, in which the parameters and structure of a neural network are evolved through several generations to yield better networks.

**Grid Search**

Grid search is actually exhaustive search over a discrete grid of parameters, is the most common approach to hyperparameter optimization used in machine learning.

In a grid search, you define which parameters you want to optimize for and
what values you want to test for each parameter.

Grid search will try combinations of your hyperparameters and return the combination which yields the best model.
