<!-- livebook:{"persist_outputs":true} -->

# Chapter 7. Learn to See

```elixir
Mix.install([
{:axon, "~> 0.5"},
{:nx, "~> 0.5"},
{:exla, "~> 0.5"},
{:stb_image, "~> 0.6"},
{:kino, "~> 0.8"}
])
```

## Identifying Cats and Dogs

Download the data set at: [KAGGLE.com](https://www.kaggle.com/competitions/dogs-vs-cats/data)

```elixir
# Lets speed up
Nx.global_default_backend(EXLA.Backend)
```

<!-- livebook:{"output":true} -->

```
{Nx.BinaryBackend, []}
```

## Building an Input Pipeline

Elixir streams are often more performant as training input pipelines, especially when using an accelerator, such as a GPU. And have the following
advantage:

**Memory Efficiency**

Practical datasets are often too large to fit entirely in memory. Streams only yield results when requested, which means you can consume batches of images one-by-one and avoid loading an entire dataset into memory.

**Overlapping Execution**

When using an external accelerator, such as a GPU, for training, the CPU is often idle for long periods of time as its only responsibility is feeding inputs to the GPU. GPUs are so fast that data transfer is often the most
expensive operation.

GPU starvation happens when the input pipeline is IO-bound rather than compute bound.

You can combine streams with some of Elixir’s concurrency primitives to create pipelines that maximize both the GPU and CPU usage.

```elixir
# Lazy pipeline

defmodule CatsAndDogs do
  def pipeline(paths, batch_size, target_height, target_width) do
    paths
    |> Enum.shuffle()
    |> Task.async_stream(&parse_image/1)
    |> Stream.filter(fn
      {:ok, {%StbImage{}, _}} -> true
      _ -> false
    end)
    |> Stream.map(&to_tensors(&1, target_height, target_width))
    |> Stream.map(&random_flip(&1, :height))
    |> Stream.map(&random_flip(&1, :width))
    |> Stream.chunk_every(batch_size, batch_size, :discard)
    |> Stream.map(fn chunks ->
      {img_chunk, label_chunk} = Enum.unzip(chunks)
      # to stack the list of tensors into a single tensor.
      {Nx.stack(img_chunk), Nx.stack(label_chunk)}
    end)
  end

  defp parse_image(path) do
    label = if String.contains?(path, "cat."), do: 0, else: 1

    case StbImage.read_file(path) do
      {:ok, img} -> {img, label}
      _error -> :error
    end
  end

  defp to_tensors({:ok, {img, label}}, target_height, target_width) do
    img_tensor =
      img
      |> StbImage.resize(target_height, target_width)
      |> StbImage.to_nx()
      # |> Nx.transpose(axes: [2, 0, 1])
      |> Nx.divide(255)

    label_tensor = Nx.tensor([label])
    {img_tensor, label_tensor}
  end

  defp random_flip({image, label}, axis) do
    if :rand.uniform() < 0.5 do
      {Nx.reverse(image, axes: [axis]), label}
    else
      {image, label}
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, CatsAndDogs, <<70, 79, 82, 49, 0, 0, 16, ...>>, {:random_flip, 2}}
```

```elixir
notebook_path = "/home/alde/Documents/MyDevelopment/Machine_Learning_in_Elixir"

target_height = 96
target_width = 96
batch_size = 128

{test_paths, train_paths} =
  notebook_path
  |> Path.join("dogs-vs-cats")
  |> Path.join("train/*.jpg")
  |> Path.wildcard()
  |> Enum.shuffle()
  |> Enum.split(1000)

train_pipeline =
  CatsAndDogs.pipeline(
    train_paths,
    batch_size,
    target_height,
    target_width
  )

test_pipeline =
  CatsAndDogs.pipeline(
    test_paths,
    batch_size,
    target_height,
    target_width
  )
```

<!-- livebook:{"output":true} -->

```
#Stream<[
  enum: #Stream<[
    enum: #Function<3.111167033/2 in Task.build_stream/3>,
    funs: [#Function<40.53678557/1 in Stream.filter/2>, #Function<48.53678557/1 in Stream.map/2>,
     #Function<48.53678557/1 in Stream.map/2>, #Function<48.53678557/1 in Stream.map/2>,
     #Function<3.53678557/1 in Stream.chunk_while/4>]
  ]>,
  funs: [#Function<48.53678557/1 in Stream.map/2>]
]>
```

```elixir
Enum.take(train_pipeline, 1)
```

<!-- livebook:{"output":true} -->

```
[
  {#Nx.Tensor<
     f32[128][height: 96][width: 96][channels: 3]
     EXLA.Backend<host:0, 0.743397396.4011720728.249942>
     [
       [
         [
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           [0.9960784316062927, 0.9960784316062927, 0.9960784316062927],
           ...
         ],
         ...
       ],
       ...
     ]
   >,
   #Nx.Tensor<
     s64[128][1]
     EXLA.Backend<host:0, 0.743397396.4011720728.250071>
     [
       [0],
       [1],
       [0],
       [1],
       [1],
       [1],
       [1],
       [1],
       [0],
       [0],
       [1],
       [1],
       [0],
       [0],
       [0],
       [1],
       [1],
       [0],
       [1],
       [0],
       [1],
       [0],
       [1],
       [0],
       [0],
       [1],
       [0],
       [1],
       [1],
       [1],
       [1],
       [1],
       [1],
       [0],
       [0],
       [0],
       [0],
       [0],
       [1],
       [0],
       [0],
       [1],
       [1],
       [1],
       [1],
       [1],
       [1],
       ...
     ]
   >}
]
```

<!-- livebook:{"branch_parent_index":1} -->

## MLP

```elixir
mlp_model =
  Axon.input("images", shape: {nil, target_height, target_width, 3})
  # Axon.flatten takes a two or more dimensional input and flattens
  # the trailing dimensions into a single dimension.
  |> Axon.flatten()
  |> Axon.dense(256, activation: :relu)
  |> Axon.dense(128, activation: :relu)
  |> Axon.dense(1, activation: :sigmoid)
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"images" => {nil, 96, 96, 3}}
  outputs: "sigmoid_0"
  nodes: 8
>
```

```elixir
mlp_trained_model_state =
  mlp_model
  # Since this is a binary classification problem, use :binary_cross_entropy.
  # Adam is a gradientdescent based algorithm that makes slight adaptations
  # to traditional gradient-descent to improve convergence.
  |> Axon.Loop.trainer(:categorical_cross_entropy, :adam)
  |> Axon.Loop.metric(:accuracy)
  |> Axon.Loop.run(train_pipeline, %{}, epochs: 5, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

01:00:03.740 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 150, accuracy: 0.4987066 loss: 0.0024936
Epoch: 1, Batch: 163, accuracy: 0.4977610 loss: 0.0010687
Epoch: 2, Batch: 176, accuracy: 0.4979255 loss: 0.0006801
Epoch: 3, Batch: 139, accuracy: 0.4976004 loss: 0.0005343
Epoch: 4, Batch: 152, accuracy: 0.4987234 loss: 0.0004156
```

<!-- livebook:{"output":true} -->

```
%{
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[256]
      EXLA.Backend<host:0, 0.3347821597.2723807255.82494>
      [-2.872542622265272e-15, -2.677069963673755e-18, 0.006001490168273449, -0.006005452014505863, 0.006005449686199427, 0.0, 0.006005264353007078, -0.006005435716360807, 0.006005304865539074, 0.006005460862070322, 0.006005456205457449, 0.006005454808473587, 0.00600544735789299, 0.006005445495247841, 0.006005443632602692, 0.006005429662764072, 0.006005453411489725, 0.006005456205457449, -0.006005456671118736, 0.006005283445119858, 0.006005418486893177, -0.006005444563925266, 0.006005418486893177, -0.006005459930747747, 0.006005448754876852, -0.006005308125168085, 0.00600528996437788, -0.006005456205457449, -0.006005423609167337, -0.0060050711035728455, -0.00600543012842536, 0.006005392875522375, 0.006005417555570602, 0.006005373317748308, 0.006005453877151012, -0.006005438975989819, 0.00600539380684495, 0.006005054339766502, 0.0, -0.0060054543428123, -0.006005408242344856, 0.0, -0.006005449220538139, 0.0, -0.006005454808473587, -0.006005446892231703, 0.006005452014505863, -0.006005458068102598, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[27648][256]
      EXLA.Backend<host:0, 0.3347821597.2723807255.82495>
      [
        [-0.0040790303610265255, -0.011861296370625496, -0.011746843345463276, -0.003016725182533264, 0.02019592374563217, -6.052394164726138e-4, -0.007077507209032774, -0.009048323146998882, 0.0025951929856091738, -0.008510063402354717, 0.002264060080051422, 0.012012246064841747, 0.01665058545768261, 0.00955267809331417, 0.0063570053316652775, 0.004197435919195414, 0.003982984460890293, 0.011996046639978886, -0.00915535632520914, 8.089385810308158e-4, -0.007051660679280758, 0.002223733812570572, 0.005710991099476814, 0.00853954628109932, 0.008614988066256046, -0.0031679023522883654, -0.007014275062829256, 0.0013735267566516995, -0.010718457400798798, -0.011449151672422886, -0.008180731907486916, 0.002115727635100484, 0.006519697140902281, 4.571720492094755e-4, -0.005118849687278271, -6.618971237912774e-4, 0.016663042828440666, -0.007159420754760504, 0.01007820013910532, 2.659521996974945e-4, 0.001145441085100174, 0.006058008875697851, 4.821323382202536e-4, -0.004656947683542967, 0.005866022780537605, 0.0016239398391917348, 0.011373105458915234, ...],
        ...
      ]
    >
  },
  "dense_1" => %{
    "bias" => #Nx.Tensor<
      f32[128]
      EXLA.Backend<host:0, 0.3347821597.2723807255.82496>
      [-1.98629876990708e-8, -0.006005453877151012, 0.006005347706377506, 0.0, -0.006005458068102598, 0.006005460396409035, -1.800947302399436e-8, 0.006005452014505863, -0.006005460862070322, 0.006005417555570602, 0.006005426868796349, -0.006005455274134874, -0.006005387753248215, -0.006005456671118736, 0.006005455274134874, 0.006005158647894859, -0.006005436647683382, 0.00600426783785224, -0.006005364470183849, -0.006005056202411652, 0.006005437113344669, -0.006005451083183289, -0.0060054543428123, -0.0060052345506846905, 0.005995164159685373, 0.006005457602441311, -0.006005437579005957, 0.006005460862070322, -0.006005445029586554, 0.006005458999425173, 0.006005409173667431, -0.006005459930747747, 0.006005459930747747, -0.006005458999425173, -0.0060054585337638855, 0.0060054585337638855, -0.006005456671118736, 0.00600545946508646, 9.816082169322726e-9, -0.006005459930747747, -0.006005446892231703, 0.006005458999425173, 0.006005450617522001, 0.006005244795233011, -0.006005431525409222, -0.006005459930747747, 0.006005448289215565, ...]
    >,
    "kernel" => #Nx.Tensor<
      f32[256][128]
      EXLA.Backend<host:0, 0.3347821597.2723807255.82497>
      [
        [-0.09050536155700684, 0.02694806456565857, -0.09827765822410583, -0.08923366665840149, 0.03198269009590149, -0.07045251131057739, -0.09775650501251221, -0.0010725855827331543, -0.04449620842933655, -0.045294374227523804, 0.10849016904830933, 0.0064559876918792725, 0.04240763187408447, 0.08059793710708618, 0.013824373483657837, 0.10680007934570312, -0.04602611064910889, 0.006300747394561768, 0.039136290550231934, 0.06857040524482727, 0.11111623048782349, 0.12125876545906067, 0.057926177978515625, 0.10145041346549988, 0.0283738374710083, -0.050323039293289185, -0.10740697383880615, 0.07511025667190552, 0.11721435189247131, 0.1062254011631012, 0.08559682965278625, 0.03438141942024231, -0.11335533857345581, -0.08361542224884033, -0.03491401672363281, -0.09456267952919006, 0.06015419960021973, -0.024346977472305298, -0.054090529680252075, -0.04451727867126465, -0.07727870345115662, 0.10854965448379517, 0.03907284140586853, -0.07630863785743713, -0.1196814775466919, -0.121509850025177, ...],
        ...
      ]
    >
  },
  "dense_2" => %{
    "bias" => #Nx.Tensor<
      f32[1]
      EXLA.Backend<host:0, 0.3347821597.2723807255.82498>
      [0.006005463656038046]
    >,
    "kernel" => #Nx.Tensor<
      f32[128][1]
      EXLA.Backend<host:0, 0.3347821597.2723807255.82499>
      [
        [-0.21408851444721222],
        [-0.05694381520152092],
        [0.19099660217761993],
        [-0.11774542182683945],
        [-0.08252287656068802],
        [0.13853661715984344],
        [-0.1941109001636505],
        [0.1279127299785614],
        [-0.1879405975341797],
        [0.04925248771905899],
        [0.02206360548734665],
        [-0.10648767650127411],
        [-0.15599845349788666],
        [-0.11188051849603653],
        [0.1504119336605072],
        [0.12392165511846542],
        [-0.019523972645401955],
        [0.04072601720690727],
        [-0.09392765164375305],
        [-0.01712111569941044],
        [0.0368802472949028],
        [-0.16540861129760742],
        [-0.18583504855632782],
        [-0.1841309517621994],
        [0.009798694401979446],
        [0.09658096730709076],
        [-0.045408621430397034],
        [0.2211829274892807],
        [-0.14785230159759521],
        [0.13986331224441528],
        [0.12327084690332413],
        [-0.1726800501346588],
        [0.1323537677526474],
        [-0.15824267268180847],
        [-0.10378176718950272],
        [0.10741936415433884],
        [-0.06700964272022247],
        [0.1452302485704422],
        [0.10699773579835892],
        [-0.15934349596500397],
        [-0.16734625399112701],
        [0.15101900696754456],
        [0.12254317849874496],
        [0.040428198873996735],
        [-0.036617666482925415],
        ...
      ]
    >
  }
}
```

```elixir
mlp_model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(test_pipeline, mlp_trained_model_state, compiler: EXLA)
# Recent advances in deep learning have suggested that adding more
# capacity to a model might have a direct positive correlation to
# model performance. Scaling the right model will lead to better performance.
```

<!-- livebook:{"output":true} -->

```

01:02:59.022 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Batch: 6, accuracy: 0.5323661
```

<!-- livebook:{"output":true} -->

```
%{
  0 => %{
    "accuracy" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3347821597.2723807255.87916>
      0.5323660969734192
    >
  }
}
```

<!-- livebook:{"branch_parent_index":1} -->

## Introducing Convolutional Neural Networks

Convolutional neural networks are neural networks that replace traditional matrix multiplications in dense layers with convolution operations.

An image is a grid of n dimensions, the *kernel* is a smaller gird and the convolution is computing the weighted sum between the input grid and the kernel creating a *Feature map*. The kernel shall traver the entire image for every valid, or window of the input (image).

The size of each step you take is often referred to as the *stride* of the convolution. In the end, you’ll have a fully transformed feature map.

```elixir
# This code uses Nx.conv to implement a basic edge detector.
image_path =
  notebook_path
  |> Path.join("dogs-vs-cats")
  |> Path.join("train/dog.5.jpg")

img =
  image_path
  |> StbImage.read_file!()
  |> StbImage.to_nx()
  |> Nx.transpose(axes: [:channels, :height, :width])
  |> Nx.new_axis(0)

kernel =
  Nx.tensor([
    [-1, 0, 1],
    [-1, 0, 1],
    [-1, 0, 1]
  ])

kernel = kernel |> Nx.reshape({1, 1, 3, 3}) |> Nx.broadcast({3, 3, 3, 3})

img
|> Nx.conv(kernel)
|> Nx.as_type({:u, 8})
|> Nx.squeeze(axes: [0])
|> Nx.transpose(axes: [:height, :width, :channels])
|> Kino.Image.new()
```

In a convolutional layer, you start with a randomly initialized kernel, and during training, the kernel starts to converge toward a parameterization capable of extracting useful features from the input.

A real convolutional kernel for image data will have four dimensions. It will have two spatial dimensions that map directly to the X and Y coordinates of an input image. A depth or input channel dimension that corresponds to the color-depth of the input image.

It will have some number of output filters. Rather than learn a single filter per kernel, it’s
useful to learn multiple filters or multiple transformations of the input image.

The output depth is considered the dimensionality of the convolutional layer.

Convolutional layers are powerful feature extractors, especially on data such as images that has a natural grid-like structure.

## The Anatomy of a CNN

A convolutional neural network is any type of neural network that contains one or more convolutional layers.

They consist of one or more blocks or stages with a convolutional layer, an activation function, and a pooling operation. These blocks make up a convolutional base. The convolutional base is typically followed by a fully-connected network that takes learned features from the convolutional base, and maps them to an output label.

**Convolutional Layers**

A typical block in a convolutional base starts with a convolutional layer that performs a linear operation and forwards activations to a non-linear activation function.

**Activation Functions**

Activation functions in the context of CNNs can be thought of as detectors, they fire on certain features.

**Pooling Layers**

Pooling layers slide a grid over an input tensor and compute an output tensor using the result of an aggregate operation for each valid spatial window in the tensor.

Pooling operations share many of the same hyperparameters which control the size and traversal of a convolutional kernel, including kernel size, padding, and strides. Pooling layers, however, do not include a learned kernel.

The intent of a pooling layer is to reduce the dimensionality of an input tensor while retaining some amount of useful information to be used in later layers.

Max pooling and average pooling are two of the most common pooling operations used in CNNs, computing the max or the average of every spatial window in the input, respectively.

**Fully-connected Head**

CNNs are commonly used as feature extractors. A fully-connected head to map features to labels. Most CNNs have a flatten or global pooling layer that aggregates the learned convolutional features into a form suitable for a dense network.

## Implementing CNNs with Axon

```elixir
cnn_model = Axon.input("images", shape: {nil, 96, 96, 3})

template = Nx.template({1, 96, 96, 3}, :f32)

Axon.Display.as_graph(cnn_model, template)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
3[/"images (:input) {1, 96, 96, 3}"/];
;
```

```elixir
cnn_model =
  cnn_model
  |> Axon.conv(32,
    kernel_size: {3, 3},
    # Padding controls how the input is altered before the convolution happens.
    padding: :same,
    activation: :relu
  )

Axon.Display.as_graph(cnn_model, template)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
3[/"images (:input) {1, 96, 96, 3}"/];
4["conv_0 (:conv) {1, 96, 96, 32}"];
5["relu_0 (:relu) {1, 96, 96, 32}"];
4 --> 5;
3 --> 4;
```

```elixir
cnn_model =
  cnn_model
  |> Axon.max_pool(kernel_size: {2, 2}, strides: [2, 2])

Axon.Display.as_graph(cnn_model, template)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
3[/"images (:input) {1, 96, 96, 3}"/];
4["conv_0 (:conv) {1, 96, 96, 32}"];
5["relu_0 (:relu) {1, 96, 96, 32}"];
6["max_pool_0 (:max_pool) {1, 48, 48, 32}"];
5 --> 6;
4 --> 5;
3 --> 4;
```

```elixir
cnn_model =
  Axon.input("images", shape: {nil, 96, 96, 3})
  |> Axon.conv(32, kernel_size: {3, 3}, activation: :relu, padding: :same)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: [2, 2])
  |> Axon.conv(64, kernel_size: {3, 3}, activation: :relu, padding: :same)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: [2, 2])
  |> Axon.conv(128, kernel_size: {3, 3}, activation: :relu, padding: :same)
  |> Axon.max_pool(kernel_size: {2, 2}, strides: [2, 2])
  |> Axon.flatten()
  |> Axon.dense(128, activation: :relu)
  |> Axon.dropout(rate: 0.5)
  |> Axon.dense(1, activation: :sigmoid)

Axon.Display.as_graph(cnn_model, template)
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
22[/"images (:input) {1, 96, 96, 3}"/];
23["conv_0 (:conv) {1, 96, 96, 32}"];
24["relu_0 (:relu) {1, 96, 96, 32}"];
25["max_pool_0 (:max_pool) {1, 48, 48, 32}"];
26["conv_1 (:conv) {1, 48, 48, 64}"];
27["relu_1 (:relu) {1, 48, 48, 64}"];
28["max_pool_1 (:max_pool) {1, 24, 24, 64}"];
29["conv_2 (:conv) {1, 24, 24, 128}"];
30["relu_2 (:relu) {1, 24, 24, 128}"];
31["max_pool_2 (:max_pool) {1, 12, 12, 128}"];
32["flatten_0 (:flatten) {1, 18432}"];
33["dense_0 (:dense) {1, 128}"];
34["relu_3 (:relu) {1, 128}"];
35["dropout_0 (:dropout) {1, 128}"];
36["dense_1 (:dense) {1, 1}"];
37["sigmoid_0 (:sigmoid) {1, 1}"];
36 --> 37;
35 --> 36;
34 --> 35;
33 --> 34;
32 --> 33;
31 --> 32;
30 --> 31;
29 --> 30;
28 --> 29;
27 --> 28;
26 --> 27;
25 --> 26;
24 --> 25;
23 --> 24;
22 --> 23;
```

```elixir
cnn_trained_model_state =
cnn_model
|> Axon.Loop.trainer(:categorical_cross_entropy, :adam)
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(train_pipeline, %{}, epochs: 5, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

16:12:26.598 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Epoch: 0, Batch: 150, accuracy: 0.5000517 loss: 0.0031380
Epoch: 1, Batch: 63, accuracy: 0.5006103 loss: 0.0018828
```

```elixir
cnn_model
|> Axon.Loop.evaluator()
|> Axon.Loop.metric(:accuracy)
|> Axon.Loop.run(test_pipeline, cnn_trained_model_state, compiler: EXLA)
```

<!-- livebook:{"output":true} -->

```

03:12:18.020 [debug] Forwarding options: [compiler: EXLA] to JIT compiler
Batch: 6, accuracy: 0.5312500
```

<!-- livebook:{"output":true} -->

```
%{
  0 => %{
    "accuracy" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.3347821597.2725904407.147155>
      0.53125
    >
  }
}
```

## Why CNNs Work

CNNs improve the performance of traditional MLPs on certain classes of input
data, such as images, because they exploit prior knowledge about the structure
of the input data.

CNNs make use of sparse interactions, parameter sharing, and equivariant representations.

The spatial dimensions of the learned kernel in a convolutional layer are
typically far smaller than the entire input image

Pooling layers further improve the performance of CNNs on certain classes
of input data by making CNNs translation invariant.

Translation invariance means that CNNs with pooling are not affected by small translations in the
input.

## Improving the Training Process

**Augmenting Data**

Data augmentation is the process of slightly modifying input data to artificially
increase the size of your dataset.

**Tweaking the Model**

Regularization is any strategy applied to the training process designed to improve your model’s generalization ability.

Dropout is a form of regularization that seeks to prevent a model from
overfitting by randomly masking some activations during training.

You should not apply dropout on an output layer.

**Early Stopping and Validation**

Early stopping is a regularization technique that stops model training when the model appears to start overfitting.

The holdout set is generally a small percentage of training data that is not shown to the model
during training. If the model’s performance on the validation data starts to
dip, you know the model is probably starting to overfit, and you can stop
training.
